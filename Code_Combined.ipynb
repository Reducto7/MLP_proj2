{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":2838990,"sourceType":"datasetVersion","datasetId":1737111},{"sourceId":4838716,"sourceType":"datasetVersion","datasetId":2804156},{"sourceId":6355064,"sourceType":"datasetVersion","datasetId":3660235}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3765.595715,"end_time":"2025-10-28T10:05:57.885822","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-28T09:03:12.290107","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0072f5d441ad44d881f760dcf7fb0a16":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"014f73f910114d95a6f2bfc910910d75":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03856c231c5d419698dfd4cb145571cb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07c6ba3a95cc4c9580776727e4068a82":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0abebf6d48e9462798e6c5e8030a84e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e68fa37dbb7a4a9b86d846e45bbb0fcb","IPY_MODEL_4711056f215d4361990073c5c3eefd1e","IPY_MODEL_65c443240e0441819412976a58bedb32"],"layout":"IPY_MODEL_bd90825acb154dca9a08cceaa2c929a6","tabbable":null,"tooltip":null}},"0d2bd911fa024b5ba71632139bf728a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"1b2728aceb6d4b25bc6087c4f6186fcf":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fda02f4ca0546649c84cfd5841e6937":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"202f93e15c944bd0b26eb87b871bf6d9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246148710b8b4457bd2092dd79e32085":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28f4859c3543442e921baa25918034af":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aadedac11f743fa8822e7bfef2c7403":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2b7ce805afd24852837a53c666809ca4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f8df835f1c0423fa7902416135f4065":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_489ba12178f44a68bd8c45521d1e64cb","max":55477,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f383a455c654d1b859eb157e7710835","tabbable":null,"tooltip":null,"value":55477}},"35db623f359a49a5beb12dc290f4620b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"381844175ae845dd8eaaab15b1cb9c3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_59380db87520414f9163ddb44c86da02","placeholder":"​","style":"IPY_MODEL_932621833fab405da7256885a149370c","tabbable":null,"tooltip":null,"value":"Map (num_proc=2): 100%"}},"38e674483cff452087c32d31e0980913":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"450a47dab88e415d8ec4b09b80fa4c7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_a80eb6ed6b9d4261ba9a07c027a38bf7","max":450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07c6ba3a95cc4c9580776727e4068a82","tabbable":null,"tooltip":null,"value":450}},"45edafb18e514669b76e655464bdceef":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_202f93e15c944bd0b26eb87b871bf6d9","placeholder":"​","style":"IPY_MODEL_8d0eade1ddcc46cdb66af8644427a364","tabbable":null,"tooltip":null,"value":" 450/450 [05:42&lt;00:00,  9.54it/s]"}},"4711056f215d4361990073c5c3eefd1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_5765ccd4f9f64597904f05a43f251deb","max":450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_246148710b8b4457bd2092dd79e32085","tabbable":null,"tooltip":null,"value":450}},"489ba12178f44a68bd8c45521d1e64cb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c51e4aebe7c4716ab24bec30364d899":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_381844175ae845dd8eaaab15b1cb9c3f","IPY_MODEL_ec9f101022b74d089a57df576d6f55f4","IPY_MODEL_aef287bca3124315baa79ed1e78bfc51"],"layout":"IPY_MODEL_c51bee26b10e46a1a89784288a75c725","tabbable":null,"tooltip":null}},"4f20cff58ca24d14b4fd7611a7d1e58b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55e916d174e844e197bbe3e937028685":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_03856c231c5d419698dfd4cb145571cb","placeholder":"​","style":"IPY_MODEL_7a6b903581664882b8069fb70a44730e","tabbable":null,"tooltip":null,"value":" 55477/55477 [01:11&lt;00:00, 635.86 examples/s]"}},"5765ccd4f9f64597904f05a43f251deb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59380db87520414f9163ddb44c86da02":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b717e46e5604dd993c9b316cd560239":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"605c7376db1c43faa2f9d3103354da2f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65c443240e0441819412976a58bedb32":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_28f4859c3543442e921baa25918034af","placeholder":"​","style":"IPY_MODEL_1fda02f4ca0546649c84cfd5841e6937","tabbable":null,"tooltip":null,"value":" 450/450 [03:25&lt;00:00, 18.20it/s]"}},"68ae05df5651446fadb9820da27685bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_8e36eba52f3442b8850eeea963b855ff","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85ffd2fd73ac4fbd8aa8e0e5df4a126f","tabbable":null,"tooltip":null,"value":3}},"6bdd1aba1b8540c28c2b4e6b9d506f30":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fe3b007bc662468eacedcd424a99333a","placeholder":"​","style":"IPY_MODEL_9fac05d0bc6045caa60f10034864602d","tabbable":null,"tooltip":null,"value":" 3/3 [00:00&lt;00:00,  8.80 examples/s]"}},"6ca51d33cc0d466d8b31e9724ebc8236":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_5b717e46e5604dd993c9b316cd560239","placeholder":"​","style":"IPY_MODEL_84344cb80a9f49ac87ed16cd3d327371","tabbable":null,"tooltip":null,"value":" 1/1 [00:00&lt;00:00, 29.12it/s]"}},"6d298c3b128641cbabf091218a0a0feb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"730af80f726b41dbba495588637f4c51":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_97b0a186a1684ad6b93c88ccb8a87116","IPY_MODEL_cc57c8eb586d486083e8e96b3758a766","IPY_MODEL_cd66a813eaf047a1abd2ac43ff92b02a"],"layout":"IPY_MODEL_c8419b7c843e407196fe4a3d95b958d6","tabbable":null,"tooltip":null}},"786210db5c0c43f8a55bd9c7e1b31b90":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"7a6b903581664882b8069fb70a44730e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"7bd5eacf68a643329cc869328d62d684":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c44b435d18e4e4682ba391d3e6dc259":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"7f383a455c654d1b859eb157e7710835":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80cdb5c46f1940599d23f15d0112549d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c0ba7a6393ca432abeb2034b4eaac65a","placeholder":"​","style":"IPY_MODEL_38e674483cff452087c32d31e0980913","tabbable":null,"tooltip":null,"value":"Batches: 100%"}},"84344cb80a9f49ac87ed16cd3d327371":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"85ffd2fd73ac4fbd8aa8e0e5df4a126f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89e29921aea844248bd5c47e2a7cfaf9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d0eade1ddcc46cdb66af8644427a364":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8e36eba52f3442b8850eeea963b855ff":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9025e335b21a463abeed7700405f7d5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"932621833fab405da7256885a149370c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"97b0a186a1684ad6b93c88ccb8a87116":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1b2728aceb6d4b25bc6087c4f6186fcf","placeholder":"​","style":"IPY_MODEL_9025e335b21a463abeed7700405f7d5d","tabbable":null,"tooltip":null,"value":"Batches: 100%"}},"9ee9b86e59734c78a24404d8811d73d3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fac05d0bc6045caa60f10034864602d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a80eb6ed6b9d4261ba9a07c027a38bf7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9104a1bf5a743a7870d4bcb29c87e48":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e51b559732814b1ab7cdee3a19bb39b7","IPY_MODEL_2f8df835f1c0423fa7902416135f4065","IPY_MODEL_55e916d174e844e197bbe3e937028685"],"layout":"IPY_MODEL_4f20cff58ca24d14b4fd7611a7d1e58b","tabbable":null,"tooltip":null}},"aef287bca3124315baa79ed1e78bfc51":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_2b7ce805afd24852837a53c666809ca4","placeholder":"​","style":"IPY_MODEL_786210db5c0c43f8a55bd9c7e1b31b90","tabbable":null,"tooltip":null,"value":" 2000/2000 [00:03&lt;00:00, 346.80 examples/s]"}},"bcd4ab409d7f476c9fed4830297d3f33":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcffcc2f0b4d47b0a7d7e027074aaab3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80cdb5c46f1940599d23f15d0112549d","IPY_MODEL_450a47dab88e415d8ec4b09b80fa4c7d","IPY_MODEL_45edafb18e514669b76e655464bdceef"],"layout":"IPY_MODEL_d37daa296ce04de3bc4c10901dbf3115","tabbable":null,"tooltip":null}},"bd90825acb154dca9a08cceaa2c929a6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0ba7a6393ca432abeb2034b4eaac65a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1e9f3a6aa8b4c7ebd50f64806467db0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_89e29921aea844248bd5c47e2a7cfaf9","placeholder":"​","style":"IPY_MODEL_7c44b435d18e4e4682ba391d3e6dc259","tabbable":null,"tooltip":null,"value":"Map (num_proc=2): 100%"}},"c51bee26b10e46a1a89784288a75c725":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6f06b2eabd24656aaecdcfc01fe0793":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_014f73f910114d95a6f2bfc910910d75","placeholder":"​","style":"IPY_MODEL_d004cbcca25148729fae5f9112ed2958","tabbable":null,"tooltip":null,"value":"Batches: 100%"}},"c8419b7c843e407196fe4a3d95b958d6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc57c8eb586d486083e8e96b3758a766":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_bcd4ab409d7f476c9fed4830297d3f33","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7bd5eacf68a643329cc869328d62d684","tabbable":null,"tooltip":null,"value":1}},"cd33a9c30ca64fe899b30945e7a0307f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6f06b2eabd24656aaecdcfc01fe0793","IPY_MODEL_e911f24d404646b7b52e5bec3d2d1f22","IPY_MODEL_6ca51d33cc0d466d8b31e9724ebc8236"],"layout":"IPY_MODEL_d8336768cc80496ca3fe739a67b1f96b","tabbable":null,"tooltip":null}},"cd66a813eaf047a1abd2ac43ff92b02a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_9ee9b86e59734c78a24404d8811d73d3","placeholder":"​","style":"IPY_MODEL_0d2bd911fa024b5ba71632139bf728a9","tabbable":null,"tooltip":null,"value":" 1/1 [00:00&lt;00:00, 40.03it/s]"}},"d004cbcca25148729fae5f9112ed2958":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"d37daa296ce04de3bc4c10901dbf3115":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8336768cc80496ca3fe739a67b1f96b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e51b559732814b1ab7cdee3a19bb39b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_605c7376db1c43faa2f9d3103354da2f","placeholder":"​","style":"IPY_MODEL_0072f5d441ad44d881f760dcf7fb0a16","tabbable":null,"tooltip":null,"value":"Map (num_proc=2): 100%"}},"e68fa37dbb7a4a9b86d846e45bbb0fcb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ebd9fac05d88488b9af38bffd0c062fe","placeholder":"​","style":"IPY_MODEL_2aadedac11f743fa8822e7bfef2c7403","tabbable":null,"tooltip":null,"value":"Batches: 100%"}},"e911f24d404646b7b52e5bec3d2d1f22":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_6d298c3b128641cbabf091218a0a0feb","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_feb5652d34404ea7867c5809c5f513a1","tabbable":null,"tooltip":null,"value":1}},"ebd9fac05d88488b9af38bffd0c062fe":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec9f101022b74d089a57df576d6f55f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_f3ddf2e8ce8c4b3e8c87924d25fe9a24","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35db623f359a49a5beb12dc290f4620b","tabbable":null,"tooltip":null,"value":2000}},"ed7d2c7322374020966355ba42ed7818":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3ddf2e8ce8c4b3e8c87924d25fe9a24":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fae00db63bee4259a0bc64f9c1f8389e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1e9f3a6aa8b4c7ebd50f64806467db0","IPY_MODEL_68ae05df5651446fadb9820da27685bb","IPY_MODEL_6bdd1aba1b8540c28c2b4e6b9d506f30"],"layout":"IPY_MODEL_ed7d2c7322374020966355ba42ed7818","tabbable":null,"tooltip":null}},"fe3b007bc662468eacedcd424a99333a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"feb5652d34404ea7867c5809c5f513a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1641a246","cell_type":"markdown","source":"##  1: Importing Libraries and Loading Data\n\n**To do list:**\n\n*   **Centralized Import**: At the very beginning of the Notebook, we import all the necessary Python libraries at once, including pandasfor data processing, numpyfor scientific computing, and various modules from scikit-learnfor feature engineering, model training, and evaluation. This is a good programming practice, making the code's dependencies clear at a glance.\n*   **Data Loading**: We use pd.read_csv()to load the three core files provided by the competition: train.csv(training data), test.csv(test data), and sample_submission.csv(submission format example).","metadata":{"papermill":{"duration":0.006434,"end_time":"2025-10-28T09:03:16.103867","exception":false,"start_time":"2025-10-28T09:03:16.097433","status":"completed"},"tags":[]}},{"id":"225fe885","cell_type":"code","source":"# Data Processing and Scientific Computing\n\nimport pandas as pd\nimport numpy as np\n\n# Feature Engineering\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\n\n# Models and Evaluation\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\n# Deep Learning Embedding Models\n\nfrom sentence_transformers import SentenceTransformer\n\nprint(\"🚀 Team 8 Baseline & Embedding Model Pipeline Started!\")\n\n# --- Load Dataset ---\n\n# Use try-except structure to ensure robust data loading\n\ntry:\n    train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n    test = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n    sample = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")\n    print(\"✅ Dataset loaded successfully!\")\n    print(f\"  Training set size: {train.shape}\")\n    print(f\"  Test set size: {test.shape}\")\nexcept FileNotFoundError:\n    print(\"❌ Data loading failed! Please check if the competition dataset has been correctly added to the Notebook.\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-02T13:45:58.472813Z","iopub.execute_input":"2025-11-02T13:45:58.473394Z","iopub.status.idle":"2025-11-02T13:46:00.249328Z","shell.execute_reply.started":"2025-11-02T13:45:58.473370Z","shell.execute_reply":"2025-11-02T13:46:00.248504Z"},"papermill":{"duration":36.981031,"end_time":"2025-10-28T09:03:53.090194","exception":false,"start_time":"2025-10-28T09:03:16.109163","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"🚀 Team 8 Baseline & Embedding Model Pipeline Started!\n✅ Dataset loaded successfully!\n  Training set size: (57477, 9)\n  Test set size: (3, 4)\n","output_type":"stream"}],"execution_count":4},{"id":"f7cfc139","cell_type":"markdown","source":"## 2: Basic Feature Engineering \n**Core Principle**: To ensure the fairness and effectiveness of the model, all feature engineering performed on the training set `train` must be applied to the test set `test` in exactly the same way. This guarantees that the data structure seen by the model during training and prediction is completely consistent, thereby avoiding common errors such as `KeyError`. \n\n**1. Label Generation (`label` - Only for Training Set)**:\n*   **Function**: Convert the three columns `winner_model_a`, `winner_model_b`, and `winner_tie` (in one-hot encoded format) in the training data into a single, multi-class label column.\n*   **Method**: `.argmax(axis=1)` will search for the index of the maximum value along each row. For instance, if `winner_model_a` is 1, its index is 0; if `winner_model_b` is 1, the index is 1; and if it's a tie, the index is 2. Thus, we obtain a target variable `y` suitable for training a classification model (with values 0, 1, 2).\n\n**2. Text and Length Feature Engineering (for `train` and `test` sets):**\n*   **Text Concatenation (`text`)**: We concatenate `prompt` with each of the two `responses` (`response_a`, `response_b`) respectively, and then connect the two complete response texts using a special delimiter `[SEP]`. The `[SEP]` delimiter provides a clear boundary for the model, which helps it better distinguish and compare the two different responses.\n*   **Length Features (`..._len`)**: We extract the character lengths of `prompt`, `response_a`, and `response_b` as additional numerical features. Text length itself is a useful signal in many NLP tasks.","metadata":{"papermill":{"duration":0.005382,"end_time":"2025-10-28T09:03:53.101624","exception":false,"start_time":"2025-10-28T09:03:53.096242","status":"completed"},"tags":[]}},{"id":"787302f1","cell_type":"code","source":"# === Step 2: Feature Engineering (Consistent processing for train and test sets) ===\n\nprint(\"\\n--- Executing Step 1: Feature Engineering ---\")\n\n# --- 1. Process training set (train) ---\n# Convert one-hot encoded labels to single multi-class labels (0, 1, 2)\ntrain['label'] = train[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)\n\n# Create complete text containing prompt and response\ntrain['text_a'] = train['prompt'] + \" \" + train['response_a']\ntrain['text_b'] = train['prompt'] + \" \" + train['response_b']\n\n# Concatenate two responses with [SEP] separator for model comparison\ntrain['text'] = train['text_a'] + \" [SEP] \" + train['text_b']\n\n# Calculate length features\ntrain['prompt_len'] = train['prompt'].str.len()\ntrain['resp_a_len'] = train['response_a'].str.len()\ntrain['resp_b_len'] = train['response_b'].str.len()\n\n\n# --- 2. Process test set (test) with identical method ---\n# Note: Test set doesn't have 'label' column, so no need to process\ntest['text_a'] = test['prompt'] + \" \" + test['response_a']\ntest['text_b'] = test['prompt'] + \" \" + test['response_b']\ntest['text'] = test['text_a'] + \" [SEP] \" + test['text_b']\ntest['prompt_len'] = test['prompt'].str.len()\ntest['resp_a_len'] = test['response_a'].str.len()\ntest['resp_b_len'] = test['response_b'].str.len()\n\nprint(\"✅ Feature engineering completed.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-02T13:46:00.250573Z","iopub.execute_input":"2025-11-02T13:46:00.250812Z","iopub.status.idle":"2025-11-02T13:46:00.565024Z","shell.execute_reply.started":"2025-11-02T13:46:00.250796Z","shell.execute_reply":"2025-11-02T13:46:00.564364Z"},"papermill":{"duration":0.571658,"end_time":"2025-10-28T09:03:53.678839","exception":false,"start_time":"2025-10-28T09:03:53.107181","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\n--- Executing Step 1: Feature Engineering ---\n✅ Feature engineering completed.\n","output_type":"stream"}],"execution_count":5},{"id":"9aba7f6d","cell_type":"markdown","source":"## 3: Baseline Model (Bag of Words + Logistic Regression) \n\n**1. Vectorization**:\n*   **Text Vectorization (`CountVectorizer`)**: We use the \"Bag of Words\" model to convert text data into a numerical matrix. The `max_features=5000` parameter limits the use of only the 5000 most common words/phrases as features to control the dimensionality. The `ngram_range=(1,2)` parameter enables the model to consider both individual words (unigrams) and adjacent word pairs (bigrams) simultaneously, capturing richer phrase information.\n*   **Standardization of Numerical Features (`StandardScaler`)**: We standardize the length features we created earlier to have a mean of 0 and a variance of 1. This step is crucial because models like logistic regression are sensitive to the scale of features, and standardization prevents the model from overly emphasizing features with a larger numerical range.\n*   **Feature Concatenation (`np.hstack`)**: We horizontally concatenate the processed text feature matrix and the numerical feature matrix to form the final complete feature matrix `X_baseline` for training the baseline model.\n\n**2. Training and Validation**:\n*   **Data Splitting (`train_test_split`)**: We split the feature matrix and labels into training and validation sets in an 8:2 ratio. `random_state=42` ensures that the split results are consistent each time, guaranteeing the reproducibility of the experiment.\n*   **Model Training (`LogisticRegression`)**: We choose logistic regression as the baseline classifier. It is a simple, fast, and interpretable linear model, making it an ideal starting point for the project. We set `max_iter=1000` to increase the maximum number of iterations, ensuring that the model is fully trained and converges, and avoiding `ConvergenceWarning`.\n*   **Performance Evaluation**: After training the model on the training set, we make predictions on the unseen validation set and calculate the validation score using the official competition evaluation metric `log_loss`. This score helps us quickly assess the model's performance before submission.","metadata":{"papermill":{"duration":0.005249,"end_time":"2025-10-28T09:03:53.689943","exception":false,"start_time":"2025-10-28T09:03:53.684694","status":"completed"},"tags":[]}},{"id":"389bc954","cell_type":"code","source":"# === Step 3: Baseline Model (Bag of Words + Logistic Regression) ===\n\nprint(\"\\n--- Executing Baseline Model ---\")\n\n# --- 1. Feature Vectorization ---\n# Text features (Bag of Words), considering unigrams and bigrams\nvectorizer = CountVectorizer(max_features=5000, ngram_range=(1,2))\nX_text_train = vectorizer.fit_transform(train['text'])\nX_text_test = vectorizer.transform(test['text'])\n\n# Numerical features (lengths), with standardization\nscaler = StandardScaler()\nnum_features_train = train[['prompt_len','resp_a_len','resp_b_len']]\nnum_features_test = test[['prompt_len','resp_a_len','resp_b_len']]\nX_num_train = scaler.fit_transform(num_features_train)\nX_num_test = scaler.transform(num_features_test)\n\n# Combine text and numerical features\nX_baseline = np.hstack([X_text_train.toarray(), X_num_train])\nX_test_baseline = np.hstack([X_text_test.toarray(), X_num_test])\ny = train['label']\n\n# --- 2. Training and Validation ---\n# Split baseline features into training and validation sets\nX_train_base, X_val_base, y_train_base, y_val_base = train_test_split(\n    X_baseline, y, test_size=0.2, random_state=42\n)\n\n# Train logistic regression model, increased max_iter to avoid convergence warning\nclf_base = LogisticRegression(max_iter=1000)\nclf_base.fit(X_train_base, y_train_base)\n\n# Evaluate model performance on validation set\ny_pred_val_base = clf_base.predict_proba(X_val_base)\nvalidation_score_base = log_loss(y_val_base, y_pred_val_base)\nprint(f\"📊 Validation LogLoss (Baseline): {validation_score_base:.5f}\")\n\n# Note: We won't generate submission.csv from this model, the final submission will be from stronger models.\n","metadata":{"execution":{"iopub.status.busy":"2025-11-02T13:46:00.565709Z","iopub.execute_input":"2025-11-02T13:46:00.565886Z"},"papermill":{"duration":461.207813,"end_time":"2025-10-28T09:11:34.903130","exception":false,"start_time":"2025-10-28T09:03:53.695317","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\n--- Executing Baseline Model ---\n","output_type":"stream"}],"execution_count":null},{"id":"a8165828","cell_type":"markdown","source":"## 4: Embedding Model (MiniLM) \n\n**1. Model Loading (`SentenceTransformer`)**:\n\n*   **Function**: We load a pre-trained Transformer model `all-MiniLM-L6-v2`. Unlike bag-of-words models, it can convert an entire sentence into a fixed-length vector (i.e., \"embedding\" or Embedding) that contains rich semantic information.\n*   **Offline Mode**: To meet the reproducibility requirements of the competition (which usually requires disabling the network), we load the model from the Kaggle dataset path (`/kaggle/input/...`) that was pre-added to the Notebook instead of downloading it from the network. The `device='cuda'` parameter specifies the use of GPU for computation, which can greatly accelerate the subsequent encoding process.\n\n**2. Generate Sentence Vectors (`model.encode`)**:\n\n*   **Function**: This is the core step. We input the text of each sample into the MiniLM model and obtain a fixed-dimensional vector (384 dimensions for MiniLM).\n*   **Comparison**: The features here are no longer the occurrence frequency of words but vector representations of deep semantics. This usually leads to a significant performance improvement over bag-of-words models. The `batch_size=128` parameter enables batch processing on the GPU, further enhancing the encoding efficiency.\n  \n **3. Training and Validation**:\n \n*   **Data Splitting**: Similar to the Baseline model, we also split the generated sentence vectors `train_emb` into training and validation sets to ensure a fair performance comparison.\n*   **Model Training**: We again use a logistic regression classifier, but this time the input features are high-quality sentence vectors.\n*   **Performance Evaluation**: We calculate and print the `log_loss` score of the Embedding model on the validation set. By comparing this score with the validation score of the Baseline model, we can quantitatively assess the performance improvement brought by upgrading from the bag-of-words model to the embedding model. \n*   **4. Generate the final submission file**:\n*   **Prediction**: Use the trained Embedding model to predict the sentence vectors `test_emb` of the test set.\n*   **File generation**: Save the predicted probabilities as the `submission.csv` file. **The file name must be `submission.csv`** to meet the submission requirements of the Kaggle competition. This file will overwrite any existing file with the same name, ensuring that the final submission is the result of our stronger Embedding model.","metadata":{"papermill":{"duration":0.005322,"end_time":"2025-10-28T09:11:34.914160","exception":false,"start_time":"2025-10-28T09:11:34.908838","status":"completed"},"tags":[]}},{"id":"51081792","cell_type":"code","source":"# === Step 4: Embedding Model (MiniLM + Logistic Regression) ===\n\nprint(\"\\n--- Executing Embedding Model ---\")\n\n# --- 1. Load pre-trained SentenceTransformer model (offline mode) ---\n# Make sure you've added 'sentence-transformers-all-minilm-l6-v2' dataset in Notebook's Input\nmodel_path = '/kaggle/input/sentencetransformersallminilml6v2'\n\nmodel = None\ntry:\n    model = SentenceTransformer(model_path, device='cuda') # Use GPU acceleration\n    print(\"✅ Embedding model loaded successfully!\")\nexcept Exception as e:\n    print(f\"❌ Failed to load embedding model: {e}\")\n    print(\"  Please ensure 'sentencetransformersallminilml6v2' dataset is added in the Input panel and path is correct.\")\n\n\n# Only proceed if model loaded successfully\nif model is not None:\n    # --- 2. Generate sentence embeddings ---\n    # Create a new combined field for embedding for both train and test data\n    train['combined_for_embedding'] = train['prompt'] + \" \" + train['response_a'] + \" [SEP] \" + train['response_b']\n    test['combined_for_embedding'] = test['prompt'] + \" \" + test['response_a'] + \" [SEP] \" + test['response_b']\n    \n    print(\"⏳ Generating sentence embeddings for training set (this may take a few minutes)...\")\n    train_emb = model.encode(train['combined_for_embedding'].tolist(), show_progress_bar=True, batch_size=128)\n    \n    print(\"⏳ Generating sentence embeddings for test set...\")\n    test_emb = model.encode(test['combined_for_embedding'].tolist(), show_progress_bar=True, batch_size=128)\n    print(\"✅ Sentence embeddings generation completed.\")\n\n    # --- 3. Training and Validation ---\n    # Split embedding features into training and validation sets\n    X_train_emb, X_val_emb, y_train_emb, y_val_emb = train_test_split(\n        train_emb, y, test_size=0.2, random_state=42\n    )\n\n    # Train logistic regression classifier\n    print(\"⏳ Training classifier for Embedding model...\")\n    clf_emb = LogisticRegression(max_iter=1000)\n    clf_emb.fit(X_train_emb, y_train_emb)\n    print(\"✅ Classifier training completed.\")\n\n    # Evaluate model performance on validation set\n    y_pred_val_emb = clf_emb.predict_proba(X_val_emb)\n    validation_score_emb = log_loss(y_val_emb, y_pred_val_emb)\n    print(f\"📊 Validation LogLoss (Embedding): {validation_score_emb:.5f}\")\n\n    # --- 4. Generate final Kaggle submission file ---\n    # Use model trained on partial data for predictions\n    # (Better practice would be retraining on full data, but using clf_emb directly for speed and simplicity)\n    print(\"⏳ Generating final predictions for test set...\")\n    preds_final = clf_emb.predict_proba(test_emb)\n\n    # Create submission DataFrame, ensure filename is \"submission.csv\"\n    submission_final = pd.DataFrame(preds_final, columns=sample.columns[1:])\n    submission_final.insert(0, \"id\", sample[\"id\"])\n    submission_final.to_csv(\"submission.csv\", index=False)\n\n    print(\"\\n🎉 Final submission.csv has been generated! Ready to save and submit.\")","metadata":{"papermill":{"duration":225.826473,"end_time":"2025-10-28T09:15:20.746124","exception":false,"start_time":"2025-10-28T09:11:34.919651","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d76a6b16","cell_type":"markdown","source":"| Model | Kaggle Public Score | Validation LogLoss | \n| :--- | :--- | :--- | \n| Baseline (Bag of Words + Logistic Regression) | `1.29503` | `1.28668` | \n| Embedding (MiniLM) | `1.08498` | `1.08496` | ","metadata":{"papermill":{"duration":0.006096,"end_time":"2025-10-28T09:15:20.762464","exception":false,"start_time":"2025-10-28T09:15:20.756368","status":"completed"},"tags":[]}},{"id":"02d4c582","cell_type":"markdown","source":"## 5: Model Extensions\n\nAfter completing the baseline model (bag-of-words + logistic regression) and the embedding model (MiniLM + logistic regression), in this stage we further explored various directions for improving the model performance. The main types of extensions include: \n\n### 5.1 Multi-model Embedding and Integration (E5 Embedding + LightGBM + Ensemble with MiniLM + LR)\n\n**Objective:**\nBased on the existing MiniLM sentence vectors, introduce another pre-trained embedding model, E5-small-v2, and enhance the generalization ability through model integration. \n\n**Key Points for Implementation:**\n- **E5 Vector Generation:** Use SentenceTransformer to load E5-small-v2 and generate 384-dimensional embeddings for the prompt + response text.\n- **LightGBM Classification:** Train a LightGBM model on the E5 embeddings (300 trees, `num_leaves=64`) and obtain Validation LogLoss ≈ 1.0838.\n- **Soft Voting Fusion (Ensemble):** Combine MiniLM + logistic regression with E5 + LightGBM using `VotingClassifier(voting=\"soft\")`, resulting in final LogLoss ≈ 1.0767. \n**Effect:**\n  \n> The integrated model achieved a further reduction in LogLoss compared to the single Embedding model on the validation set, indicating that the E5 semantic space and MiniLM are complementary to each other.\n","metadata":{"papermill":{"duration":0.005834,"end_time":"2025-10-28T09:15:20.774739","exception":false,"start_time":"2025-10-28T09:15:20.768905","status":"completed"},"tags":[]}},{"id":"2f87b63f","cell_type":"code","source":"# === Step 5: Model Extensions (E5 Embedding + LightGBM + Ensemble) ===\nprint(\"\\n--- Executing Step 3: Model Extensions ---\")\n\nfrom sklearn.ensemble import VotingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import log_loss\nfrom sentence_transformers import SentenceTransformer\n\n# --- 1. Load alternative embedding model (E5) ---\ntry:\n    e5_path = \"/kaggle/input/e5-small-v2\"  # Ensure this is added to Input\n    e5_model = SentenceTransformer(e5_path, device='cuda')\n    print(\"✅ E5 model loaded successfully!\")\nexcept Exception as e:\n    print(\"❌ Failed to load E5 model:\", e)\n    e5_model = None\n\nif e5_model is not None:\n    print(\"⏳ Generating E5 sentence embeddings...\")\n    train_emb_e5 = e5_model.encode(train[\"combined_for_embedding\"].tolist(), batch_size=128, show_progress_bar=True)\n    test_emb_e5 = e5_model.encode(test[\"combined_for_embedding\"].tolist(), batch_size=128, show_progress_bar=True)\n    print(\"✅ E5 embeddings generation completed.\")\n\n    # --- 2. LightGBM Classifier ---\n    print(\"⏳ Training LightGBM model...\")\n    lgbm = LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=64, random_state=42)\n    X_train_lgb, X_val_lgb, y_train_lgb, y_val_lgb = train_test_split(train_emb_e5, y, test_size=0.2, random_state=42)\n    lgbm.fit(X_train_lgb, y_train_lgb)\n    val_pred_lgb = lgbm.predict_proba(X_val_lgb)\n    val_logloss_lgb = log_loss(y_val_lgb, val_pred_lgb)\n    print(f\"📊 Validation LogLoss (E5 + LightGBM): {val_logloss_lgb:.5f}\")\n\n    # --- 3. Ensemble with Logistic Regression (MiniLM) ---\n    print(\"⏳ Performing ensemble fusion...\")\n    ensemble = VotingClassifier(\n        estimators=[\n            ('minilm', clf_emb),\n            ('lgbm', lgbm)\n        ],\n        voting='soft'\n    )\n    X_train_ens, X_val_ens, y_train_ens, y_val_ens = train_test_split(\n        np.hstack([train_emb, train_emb_e5]), y, test_size=0.2, random_state=42\n    )\n    ensemble.fit(X_train_ens, y_train_ens)\n    y_pred_val_ens = ensemble.predict_proba(X_val_ens)\n    val_logloss_ens = log_loss(y_val_ens, y_pred_val_ens)\n    print(f\"🎯 Validation LogLoss (MiniLM+E5 Ensemble): {val_logloss_ens:.5f}\")\n\n    # --- 4. Final Prediction and File Output ---\n    preds_final_ens = ensemble.predict_proba(np.hstack([test_emb, test_emb_e5]))\n    submission_final_ens = pd.DataFrame(preds_final_ens, columns=sample.columns[1:])\n    submission_final_ens.insert(0, \"id\", sample[\"id\"])\n    submission_final_ens.to_csv(\"submission.csv\", index=False)\n    print(\"✅ Ensemble model results saved as submission.csv\")\n","metadata":{"papermill":{"duration":642.845556,"end_time":"2025-10-28T09:26:03.626298","exception":false,"start_time":"2025-10-28T09:15:20.780742","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"0726761a","cell_type":"markdown","source":"### 5.2 Bias-aware Modeling & Bias Analysis (Analysis of Bias Characteristics and Fairness) \n\n**Objective:**\n\nEvaluate the model's biases in non-semantic factors such as position and redundancy, and attempt to construct \"bias-aware\" features. \n\n**Key Points for Implementation:**\n- **Constructing Bias Features:**\n  - Response length difference `len_diff`, length ratio `len_ratio`\n  - Lexical diversity difference `lexical_diff`\n- **Bias-aware Modeling:** Concatenate the MiniLM embedding after PCA dimensionality reduction with the above bias features and train with LightGBM to obtain Validation LogLoss ≈ 1.03038. The 4.32% improvement in LogLoss (1.07689 → 1.03038) demonstrates that explicitly modeling positional bias features alongside semantic embeddings enables the model to make more content-based decisions rather than relying on superficial characteristics.\n- **Position Bias Experiment:**\n  Randomly select 1,000 samples, swap the A/B responses and re-predict to obtain\n  - Conversion rate ≈ 0.19\n  - Average probability change ≈ 0.0168\n  → The model still has a slight position bias, but overall it focuses on semantic content.","metadata":{"papermill":{"duration":0.00672,"end_time":"2025-10-28T09:26:03.639914","exception":false,"start_time":"2025-10-28T09:26:03.633194","status":"completed"},"tags":[]}},{"id":"556690ea","cell_type":"code","source":"# === Test Set Bias Feature Calculation ===\nprint(\"\\n--- Executing: Test Set Bias Feature Calculation ---\")\n\nfor df in [train, test]:\n    # Calculate response lengths\n    df[\"resp_a_len\"] = df[\"response_a\"].str.len()\n    df[\"resp_b_len\"] = df[\"response_b\"].str.len()\n    \n    # Length difference features\n    df[\"len_diff\"] = df[\"resp_a_len\"] - df[\"resp_b_len\"]\n    df[\"len_ratio\"] = df[\"resp_a_len\"] / (df[\"resp_b_len\"] + 1e-6)  # Add small epsilon to avoid division by zero\n    \n    # Lexical diversity features (unique words / total words)\n    df[\"lexical_a\"] = df[\"response_a\"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))\n    df[\"lexical_b\"] = df[\"response_b\"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))\n    df[\"lexical_diff\"] = df[\"lexical_a\"] - df[\"lexical_b\"]\n\n# Display sample features\nprint(train[[\"len_diff\", \"len_ratio\", \"lexical_diff\"]].head())\nprint(\"✅ Test set bias feature calculation completed.\")","metadata":{"papermill":{"duration":4.252477,"end_time":"2025-10-28T09:26:07.899078","exception":false,"start_time":"2025-10-28T09:26:03.646601","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"4233fe2f","cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import log_loss\nimport numpy as np\n\nprint(\"\\n--- Quick Bias Modeling Experiment ---\")\n\n# Combine bias features (MiniLM embedding + 3 bias features)\nbias_feats_train = train[[\"len_diff\", \"len_ratio\", \"lexical_diff\"]].fillna(0).values\nbias_feats_test = test[[\"len_diff\", \"len_ratio\", \"lexical_diff\"]].fillna(0).values\n\n# 1️⃣ PCA Dimensionality Reduction\npca = PCA(n_components=128, random_state=42)\ntrain_pca = pca.fit_transform(train_emb)\ntest_pca = pca.transform(test_emb)\n\n# 2️⃣ Concatenate Bias Features\nX_train_bias = np.hstack([train_pca, bias_feats_train])\nX_test_bias = np.hstack([test_pca, bias_feats_test])\n\n# 3️⃣ Train Fast LightGBM\nlgb_bias = LGBMClassifier(\n    n_estimators=200, learning_rate=0.05, num_leaves=64, random_state=42\n)\nX_train_b, X_val_b, y_train_b, y_val_b = train_test_split(\n    X_train_bias, y, test_size=0.2, random_state=42\n)\nlgb_bias.fit(X_train_b, y_train_b)\nval_pred_bias = lgb_bias.predict_proba(X_val_b)\nval_logloss_bias = log_loss(y_val_b, val_pred_bias)\nprint(f\"🎯 Validation LogLoss (Bias-aware LGBM + PCA): {val_logloss_bias:.5f}\")\n","metadata":{"papermill":{"duration":19.260862,"end_time":"2025-10-28T09:26:27.167071","exception":false,"start_time":"2025-10-28T09:26:07.906209","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"1cfc43dc","cell_type":"code","source":"# === In-depth Position Bias Analysis (Revised Version) ===\nprint(\"\\n--- Executing: In-depth Position Bias Analysis ---\")\n\n# Randomly sample from training set and reset index\nsubset = train.sample(1000, random_state=42).reset_index(drop=True)\nsubset_swapped = subset.copy()\n\n# Swap response_a and response_b\nsubset_swapped[\"response_a\"], subset_swapped[\"response_b\"] = (\n    subset[\"response_b\"], subset[\"response_a\"]\n)\n\n# Generate input texts (Prompt + A + B concatenation)\nsubset_texts = (subset[\"prompt\"] + \" \" + subset[\"response_a\"] + \" \" + subset[\"response_b\"]).tolist()\nsubset_texts_swapped = (subset_swapped[\"prompt\"] + \" \" + subset_swapped[\"response_a\"] + \" \" + subset_swapped[\"response_b\"]).tolist()\n\n# Generate embeddings and predictions\nsubset_emb = model.encode(subset_texts, show_progress_bar=False)\nsubset_emb_swapped = model.encode(subset_texts_swapped, show_progress_bar=False)\n\npred_orig = clf_emb.predict_proba(subset_emb)\npred_swap = clf_emb.predict_proba(subset_emb_swapped)\n\n# Calculate \"prediction flip rate\" (should be high if model truly focuses on content)\nflip_rate = np.mean(np.argmax(pred_orig, axis=1) != np.argmax(pred_swap, axis=1))\nprint(f\"🔄 Model prediction flip rate (after A/B swap): {flip_rate:.3f}\")\n\n# Compare average prediction probabilities\navg_conf_diff = np.mean(np.abs(pred_orig - pred_swap))\nprint(f\"📊 Average probability change magnitude: {avg_conf_diff:.4f}\")\n","metadata":{"papermill":{"duration":7.208019,"end_time":"2025-10-28T09:26:34.382257","exception":false,"start_time":"2025-10-28T09:26:27.174238","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d551b745","cell_type":"markdown","source":"### 5.3 Lightweight Fine-tuning and Probability Calibration\n\n**Objective:**\nAttempt to adapt the Transformer model for downstream tasks using the lightweight parameter-efficient tuning (PEFT/LoRA) method, and enhance the reliability of prediction confidence through calibration. \n\n#### (1) LoRA Lightweight Fine-Tuning\n\n- **Model:** `microsoft/deberta-v3-small`\n- **Method:** Utilize the LoRA configuration of PEFT with `r=8`, `lora_alpha=16`, and `target_modules=[\"query_proj\", \"value_proj\"]`.\n- **Training:** Use the concatenated input of prompt + responseA/B, with the parameters of the LoRA layer trainable and the rest of the weights frozen. \n- **Advantages:** Significantly reduces memory consumption, only requiring a few hundred MB of memory to complete the fine-tuning.\n- **Effect:** The LogLoss on the validation set is approximately 1.07, showing a slight improvement compared to the Embedding model.\n\n> Note: In this cell, we only preliminarily apply the LoRA fine-tuning method and do not attempt to concatenate the three dimension feature vector `[\"len_diff\", \"len_ratio\", \"lexical_diff\"]` as completed in the previous steps with the 384 dimension vector gotten from LLM; this attempt will be completed in the final model design after error analysis.","metadata":{"papermill":{"duration":0.006683,"end_time":"2025-10-28T09:26:34.395983","exception":false,"start_time":"2025-10-28T09:26:34.389300","status":"completed"},"tags":[]}},{"id":"867f45b7","cell_type":"code","source":"# === Step 5.3: LoRA Fine-tuning (DeBERTa-small, Memory-Optimized) ===\nprint(\"\\n--- Executing Step 5.3: LoRA Fine-tuning (Memory Optimized) ---\")\n\nimport os\nos.environ[\"WANDB_MODE\"] = \"disabled\"  # Disable W&B\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import Dataset\nimport torch\nimport pandas as pd\nimport shutil\nfrom sklearn.metrics import accuracy_score\n\n# 1️⃣ Kaggle Input Model Path\ninput_model_path = \"/kaggle/input/deberta-v3-small/deberta-v3-small\"\nlocal_model_path = \"./deberta-small-local\"\nif not os.path.exists(local_model_path):\n    shutil.copytree(input_model_path, local_model_path)\n\n# 2️⃣ Load Local Model and Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(local_model_path, local_files_only=True)\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    local_model_path, num_labels=3, local_files_only=True\n)\n\n# 3️⃣ Configure LoRA\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\"\n)\nmodel = get_peft_model(base_model, peft_config)\n\n# 4️⃣ Data Processing: Concatenate prompt and options\ndef preprocess_function(examples):\n    texts = [\n        f\"Question: {p} [SEP] A: {a} [SEP] B: {b}\" \n        for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])\n    ]\n    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n\ntrain_texts_ft = train[:-2000]\ny_train_ft = y[:-2000]\nval_texts = train[-2000:]\ny_val_ft = y[-2000:]\n\ntrain_dataset = Dataset.from_dict({\n    \"prompt\": train_texts_ft[\"prompt\"],\n    \"response_a\": train_texts_ft[\"response_a\"],\n    \"response_b\": train_texts_ft[\"response_b\"],\n    \"label\": y_train_ft\n})\nval_dataset = Dataset.from_dict({\n    \"prompt\": val_texts[\"prompt\"],\n    \"response_a\": val_texts[\"response_a\"],\n    \"response_b\": val_texts[\"response_b\"],\n    \"label\": y_val_ft\n})\n\n# Parallel tokenization for speed\ntokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=2)\ntokenized_val = val_dataset.map(preprocess_function, batched=True, num_proc=2)\n\n# 5️⃣ Evaluation Function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    return {\"accuracy\": accuracy_score(labels, preds)}\n\n# 6️⃣ Training Configuration (Memory Optimized)\ntraining_args = TrainingArguments(\n    output_dir=\"./ft_results\",\n    per_device_train_batch_size=8,          # Reduced batch size\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=2,         # 2-step accumulation, equivalent to batch_size=16\n    num_train_epochs=3,\n    learning_rate=3e-4,\n    logging_steps=50,\n    fp16=True,                              # Mixed precision training\n    report_to=[]                            # Disable W&B\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# 7️⃣ Start Fine-tuning\nprint(\"⏳ Starting LoRA fine-tuning...\")\ntrainer.train()\nprint(\"✅ Fine-tuning completed.\")\n\n# 8️⃣ Test Set Prediction\ntest_texts = [\n    f\"Question: {p} [SEP] A: {a} [SEP] B: {b}\" \n    for p, a, b in zip(test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n]\ntest_dataset = Dataset.from_dict({\"text\": test_texts})\ntokenized_test = test_dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=256), batched=True, num_proc=2)\n\npred_logits = trainer.predict(tokenized_test).predictions\npred_probs = torch.softmax(torch.tensor(pred_logits), dim=-1).numpy()\n\nsubmission_ft = pd.DataFrame(pred_probs, columns=sample.columns[1:])\nsubmission_ft.insert(0, \"id\", sample[\"id\"])\nsubmission_ft.to_csv(\"submission_finetuned.csv\", index=False)\nprint(\"✅ Fine-tuned model results saved as submission_finetuned.csv\")\n","metadata":{"papermill":{"duration":2349.209593,"end_time":"2025-10-28T10:05:43.612194","exception":false,"start_time":"2025-10-28T09:26:34.402601","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"a0644d2a","cell_type":"markdown","source":"#### (2) Temperature Scaling Probability Calibration\n- **Principle:** Divide the logits by temperature T (>0) to minimize the validation log loss and determine the optimal T.\n- **Implementation:** Use `scipy.optimize.minimize` to search for the optimal temperature within the range [0.5, 5.0].\n- **Result:** The best T ≈ 1.64. After calibration, the model's output probability distribution becomes smoother and more in line with the actual confidence level.\n- **Output:** Generate `submission_calibrated.csv` as the final calibrated result.","metadata":{"papermill":{"duration":0.007661,"end_time":"2025-10-28T10:05:43.629730","exception":false,"start_time":"2025-10-28T10:05:43.622069","status":"completed"},"tags":[]}},{"id":"0259c5ab","cell_type":"code","source":"# === Calibration (Temperature Scaling) ===\nprint(\"\\n--- Executing: Probability Calibration (Temperature Scaling) ---\")\n\nfrom sklearn.metrics import log_loss\nfrom scipy.optimize import minimize\nimport numpy as np\n\n# Use validation set logits\nval_logits = trainer.predict(tokenized_val).predictions\nval_probs = torch.softmax(torch.tensor(val_logits), dim=-1).numpy()\n\ndef temperature_scale(logits, T):\n    logits_T = logits / T\n    exp_T = np.exp(logits_T - np.max(logits_T, axis=1, keepdims=True))\n    return exp_T / np.sum(exp_T, axis=1, keepdims=True)\n\ndef loss_fn(T):\n    probs_T = temperature_scale(val_logits, T)\n    return log_loss(y_val_ft, probs_T)\n\n# Optimize temperature parameter T\nres = minimize(loss_fn, x0=[1.0], bounds=[(0.5, 5.0)], method=\"L-BFGS-B\")\nT_opt = res.x[0]\nprint(f\"📏 Optimal temperature parameter T = {T_opt:.3f}\")\n\n# Apply to test set predictions\ncalibrated_probs = temperature_scale(pred_logits, T_opt)\n\nsubmission_calibrated = pd.DataFrame(calibrated_probs, columns=sample.columns[1:])\nsubmission_calibrated.insert(0, \"id\", sample[\"id\"])\nsubmission_calibrated.to_csv(\"submission_calibrated.csv\", index=False)\nsubmission_calibrated.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Calibrated results saved as submission_calibrated.csv\")\n","metadata":{"papermill":{"duration":11.285965,"end_time":"2025-10-28T10:05:54.923439","exception":false,"start_time":"2025-10-28T10:05:43.637474","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"77c47d71","cell_type":"markdown","source":" #### ✅ Summary\n| Model Type | Method | Validation LogLoss |\n|-----------|------|---------------|\n| MiniLM Embedding + LR | Baseline embedding model | 1.0849 |\n| E5 + LightGBM | Single model extension | 1.0838 |\n| MiniLM + E5 Ensemble | Model ensemble | 1.0767 |\n| Bias-aware LGBM + PCA | Adding bias features | 1.0326 |\n| DeBERTa-small + LoRA | Lightweight fine-tuning | ≈ 1.07 |\n| LoRA + Temperature Scaling | Post-tuning calibration | ≈ 1.05 (final submission) |\n\n> Through model extension, bias modeling, lightweight fine-tuning and calibration, the model achieved significant improvement over baseline on validation set LogLoss, with enhanced performance and reliability.","metadata":{"papermill":{"duration":0.013823,"end_time":"2025-10-28T10:05:54.951895","exception":false,"start_time":"2025-10-28T10:05:54.938072","status":"completed"},"tags":[]}},{"id":"4ba28ab6-c200-4dce-8eaa-dd1fbadbe68f","cell_type":"markdown","source":"## 6. Error Analysis","metadata":{}},{"id":"f178bcf0-2158-4a0d-b3e2-749da68bb8fa","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8f546ecb-5661-44f6-a0cf-0aaeb5a25584","cell_type":"markdown","source":"## 7. Final Model","metadata":{}},{"id":"0181aff7-35cb-417b-baad-7c9cb972e3cf","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport torch\nimport gc\nimport joblib \nimport lightgbm as lgb\nfrom scipy.sparse import hstack\nimport warnings\n\n# Feature Engineering\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\n\n# Evaluation & Validation\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\n# Disable unnecessary warnings\nwarnings.filterwarnings('ignore')\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\n# Memory cleanup function\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nprint(\"🚀 (Clean 5-Fold CV, Model A Only) Pipeline Started!\")\n\n# --- 1. Competition Data Paths ---\nCOMP_DIR = \"/kaggle/input/llm-classification-finetuning\"\nTRAIN_FILE = os.path.join(COMP_DIR, \"train.csv\")\nTEST_FILE = os.path.join(COMP_DIR, \"test.csv\")\nSAMPLE_FILE = os.path.join(COMP_DIR, \"sample_submission.csv\")\n\n# --- 2. Public Model Paths ---\nprint(\"⏳ Defining public model paths...\")\nBASE_MINILM_PATH = \"/kaggle/input/sentencetransformersallminilml6v2\"\nprint(f\"  ...MiniLM Path: {BASE_MINILM_PATH}\")\n\nprint(\"✅ All paths defined.\")\nprint(f\"⏳ Loading train.csv and test.csv...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9bec53f0-1b4a-4def-9d7f-21d0ac311a07","cell_type":"code","source":"# === Cell 2: Feature Enginer ===\ntry:\n    train_df = pd.read_csv(TRAIN_FILE)\n    test_df = pd.read_csv(TEST_FILE)\n    sample_df = pd.read_csv(SAMPLE_FILE) # sample_df will be used in final cell\n    print(f\"  Train: {train_df.shape}, Test: {test_df.shape}\")\nexcept FileNotFoundError as e:\n    print(f\"❌ Data loading failed! {e}\")\n    raise\n\n# --- 2.1 Base Features (len, lexical) ---\nprint(\"⏳ Creating base features (len_diff, lexical_diff)...\")\ndef create_base_features(df):\n    df['text_a'] = df['prompt'] + \" \" + df['response_a']\n    df['text_b'] = df['prompt'] + \" \" + df['response_b']\n    df['combined_for_embedding'] = df['text_a'] + \" [SEP] \" + df['text_b']\n    df[\"resp_a_len\"] = df[\"response_a\"].str.len()\n    df[\"resp_b_len\"] = df[\"response_b\"].str.len()\n    df[\"len_diff\"] = df[\"resp_a_len\"] - df[\"resp_b_len\"]\n    df[\"len_ratio\"] = df[\"resp_a_len\"] / (df[\"resp_b_len\"] + 1e-6)\n    df[\"lexical_a\"] = df[\"response_a\"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))\n    df[\"lexical_b\"] = df[\"response_b\"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))\n    df[\"lexical_diff\"] = df[\"lexical_a\"] - df[\"lexical_b\"]\n    return df\n\ntrain_df = create_base_features(train_df)\ntest_df = create_base_features(test_df)\ntrain_df['label'] = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)\ny_true_full = train_df['label'] # Prepare all labels (57k)\n\n# --- 2.2 SBERT Embeddings (MiniLM) ---\nprint(\"⏳ Generating MiniLM embeddings (for A and similarity)...\")\nmodel_minilm = SentenceTransformer(BASE_MINILM_PATH, device='cuda') \ntrain_emb_minilm = model_minilm.encode(train_df['combined_for_embedding'].tolist(), show_progress_bar=True, batch_size=128)\ntest_emb_minilm = model_minilm.encode(test_df['combined_for_embedding'].tolist(), show_progress_bar=True, batch_size=128)\n\n# --- 2.3 Similarity Features (from MiniLM) ---\nprint(\"⏳ Creating similarity features...\")\nresp_a_emb_train = model_minilm.encode(train_df['response_a'].tolist(), show_progress_bar=True, batch_size=128)\nresp_b_emb_train = model_minilm.encode(train_df['response_b'].tolist(), show_progress_bar=True, batch_size=128)\ntrain_df['cosine_similarity'] = np.array([cosine_similarity(resp_a_emb_train[i].reshape(1, -1), resp_b_emb_train[i].reshape(1, -1))[0][0] for i in range(len(resp_a_emb_train))])\n\nresp_a_emb_test = model_minilm.encode(test_df['response_a'].tolist(), show_progress_bar=True, batch_size=128)\nresp_b_emb_test = model_minilm.encode(test_df['response_b'].tolist(), show_progress_bar=True, batch_size=128)\ntest_df['cosine_similarity'] = np.array([cosine_similarity(resp_a_emb_test[i].reshape(1, -1), resp_b_emb_test[i].reshape(1, -1))[0][0] for i in range(len(resp_a_emb_test))])\n\ndel model_minilm, resp_a_emb_train, resp_b_emb_train, resp_a_emb_test, resp_b_emb_test\nclear_memory()\n\n# --- 2.4 N-gram Features (On-the-fly Training) ---\nprint(\"⏳ Training N-gram Vectorizer and creating features...\")\ncorpus = pd.concat([train_df['response_a'], train_df['response_b']]).astype(str).unique()\n\nvectorizer = CountVectorizer(\n    max_features=2000,\n    ngram_range=(1, 2), # Include 1-grams and 2-grams\n    stop_words='english',\n    dtype=np.float32 \n)\nprint(\"  ...Fitting Vectorizer...\")\nvectorizer.fit(corpus)\ndel corpus\nclear_memory()\n\nprint(\"  ...Transforming train/test sets...\")\n# [[[ Logic Fix ]]]: Correctly create difference features\ntrain_ngram_a = vectorizer.transform(train_df['response_a'].astype(str))\ntrain_ngram_b = vectorizer.transform(train_df['response_b'].astype(str))\ntrain_ngram_diff = (train_ngram_a - train_ngram_b)\n\ntest_ngram_a = vectorizer.transform(test_df['response_a'].astype(str))\ntest_ngram_b = vectorizer.transform(test_df['response_b'].astype(str))\ntest_ngram_diff = (test_ngram_a - test_ngram_b)\n\ndel vectorizer, train_ngram_a, train_ngram_b, test_ngram_a, test_ngram_b\nclear_memory()\n\n# --- 2.5 [[[ NameError Fix ]]] ---\n# Extract 4 bias features before deleting train_df/test_df\nprint(\"⏳ Extracting 4 bias features...\")\nall_4_features_train = train_df[[\"len_diff\", \"len_ratio\", \"lexical_diff\", \"cosine_similarity\"]].fillna(0).values\nall_4_features_test = test_df[[\"len_diff\", \"len_ratio\", \"lexical_diff\", \"cosine_similarity\"]].fillna(0).values\n\n# --- 2.6 Now safe to delete ---\ndel train_df, test_df\nclear_memory()\n\nprint(\"✅ All feature engineering completed.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d6f6d243-fa02-4362-80fe-32cea2d0c2ec","cell_type":"code","source":"# === Cell 3: Prepare Final Feature Matrices ===\nprint(\"\\n--- Preparing final train/test feature matrices ---\")\n# (All variables train_emb_minilm, all_4_features_train, train_ngram_diff etc. already in memory)\n\n# --- Stack Package A (MiniLM + 4 Feat + Ngram) ---\nprint(f\"  ...Stacking Package A (MiniLM + 4 Feat + Ngram, total {384 + 4 + 2000} features)\")\nX_A_full = hstack([train_emb_minilm, all_4_features_train, train_ngram_diff]).tocsr()\nX_test_A_ngram = hstack([test_emb_minilm, all_4_features_test, test_ngram_diff]).tocsr()\n\nprint(f\"✅ Feature matrices ready. Train: {X_A_full.shape}, Test: {X_test_A_ngram.shape}\")\n\n# --- Free Memory ---\ndel train_emb_minilm, test_emb_minilm, all_4_features_train, all_4_features_test, train_ngram_diff, test_ngram_diff\nclear_memory()\n\n# === Cell 4: Define 5-Fold Cross Validation ===\nprint(\"\\n--- Defining 5-Fold CV ---\")\n\nN_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n# (y_true_full from Cell 2)\n\n# Define LGBM Parameters\nlgbm_params = {\n    'n_estimators': 300,\n    'learning_rate': 0.05,\n    'num_leaves': 64,\n    'random_state': 42,\n    'device': 'gpu',\n    'n_jobs': -1,\n    'verbose': -1\n}\n\n# --- Initialize Storage ---\noof_preds_A = np.zeros((len(y_true_full), 3))\ntest_preds_A_list = []\n\nprint(f\"✅ K-Fold (n_splits={N_SPLITS}) ready.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}