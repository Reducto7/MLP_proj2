# === 1: å¯¼å…¥ã€è®¾ç½®ä¸å†…å­˜æ¸…ç† ===
import pandas as pd
import numpy as np
import os
import shutil
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import gc
import joblib # ç”¨äºä¿å­˜ LGBM å’Œ Sklearn æ¨¡å‹

# ç‰¹å¾å·¥ç¨‹
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity

# æ¨¡å‹ä¸è¯„ä¼°
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit # é‡‡çº³åˆ†å±‚æŠ½æ ·
from sklearn.metrics import log_loss, confusion_matrix
from lightgbm import LGBMClassifier
from scipy.optimize import minimize

# æ·±åº¦å­¦ä¹ 
from sentence_transformers import SentenceTransformer
from transformers import (
    AutoModelForSequenceClassification, AutoTokenizer, Trainer, 
    TrainingArguments, EarlyStoppingCallback # é‡‡çº³æ—©åœ
)
from peft import get_peft_model, LoraConfig, TaskType
from datasets import Dataset
from sklearn.metrics import accuracy_score

# ç¦ç”¨ wandb
os.environ["WANDB_MODE"] = "disabled"

# é‡‡çº³ä½ çš„å†…å­˜æ¸…ç†å‡½æ•°
def clear_memory():
    """æ¸…ç†GPUå’ŒCPUå†…å­˜"""
    print("\n--- æ­£åœ¨æ¸…ç†å†…å­˜ ---")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("--- å†…å­˜æ¸…ç†å®Œæ¯• ---")

print("ğŸš€ Team 8 æœ¬åœ°è®­ç»ƒç®¡çº¿å¯åŠ¨ï¼")

import os

print(f"--- ä½ çš„ Notebook æ­£åœ¨è¿™é‡Œè¿è¡Œ ---")
current_directory = os.getcwd()
print(f"å½“å‰å·¥ä½œç›®å½• (CWD): {current_directory}")

print(f"\n--- æ­£åœ¨æ£€æŸ¥æ­¤ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹ ---")

# æˆ‘ä»¬æ£€æŸ¥ 'data' å’Œ 'model' æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨äºå½“å‰ç›®å½•
data_folder_path = os.path.join(current_directory, "data")
model_folder_path = os.path.join(current_directory, "model")

if os.path.exists(data_folder_path):
    print(f"âœ… æˆåŠŸæ‰¾åˆ° 'data' æ–‡ä»¶å¤¹ï¼ è·¯å¾„: {data_folder_path}")
else:
    print(f"âŒ è­¦å‘Š: æœªæ‰¾åˆ° 'data' æ–‡ä»¶å¤¹ã€‚")

if os.path.exists(model_folder_path):
    print(f"âœ… æˆåŠŸæ‰¾åˆ° 'model' æ–‡ä»¶å¤¹ï¼ è·¯å¾„: {model_folder_path}")
else:
    print(f"âŒ è­¦å‘Š: æœªæ‰¾åˆ° 'model' æ–‡ä»¶å¤¹ã€‚")

print("\n--- æ£€æŸ¥å®Œæ¯• ---")

# === 2: å®šä¹‰æœ¬åœ°è·¯å¾„å¹¶åŠ è½½æ•°æ®é›† [å·²ä¿®å¤ Windows è·¯å¾„] ===

import pandas as pd
import os
import numpy as np # æå‰å¯¼å…¥ numpy

print("--- æ­£åœ¨è®¾ç½®æ‰€æœ‰æœ¬åœ°æ–‡ä»¶è·¯å¾„ ---")

# 1. åŸºç¡€è·¯å¾„ (æ ¹æ®ä½ çš„è¾“å‡º)
# ã€ä¿®å¤ã€‘åœ¨å­—ç¬¦ä¸²å‰æ·»åŠ  'r' æ¥åˆ›å»ºâ€œåŸå§‹å­—ç¬¦ä¸²â€ï¼Œé˜²æ­¢ \U é”™è¯¯
BASE_DIR = r"C:\Users\f1285\Desktop\ML_Project"
DATA_DIR = r"C:\Users\f1285\Desktop\ML_Project\data"
MODELS_DIR = r"C:\Users\f1285\Desktop\ML_Project\model"
OUTPUT_DIR = r"C:\Users\f1285\Desktop\ML_Project\output"

# 2. æ•°æ®æ–‡ä»¶è·¯å¾„ (os.path.join ä¼šè‡ªåŠ¨å¤„ç†æ–œæ )
TRAIN_FILE = os.path.join(DATA_DIR, "train.csv")
TEST_FILE = os.path.join(DATA_DIR, "test.csv")
SAMPLE_FILE = os.path.join(DATA_DIR, "sample_submission.csv")

# 3. åŸå§‹æ¨¡å‹è·¯å¾„
MINILM_PATH = os.path.join(MODELS_DIR, "sentencetransformersallminilml6v2")
E5_PATH = os.path.join(MODELS_DIR, "e5-small-v2")
DEBERTA_PATH = os.path.join(MODELS_DIR, "deberta-v3-small")
ROBERTA_PATH = os.path.join(MODELS_DIR, "roberta-transformers-pytorch")

# 4. æ£€æŸ¥/åˆ›å»ºè¾“å‡ºç›®å½•
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"âœ… å·²åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")
else:
    print(f"âœ… è¾“å‡ºç›®å½•å·²æ‰¾åˆ°: {OUTPUT_DIR}")

print("\n--- æ­£åœ¨åŠ è½½æ•°æ®é›† ---")

try:
    train_df = pd.read_csv(TRAIN_FILE)
    test_df = pd.read_csv(TEST_FILE)
    sample_df = pd.read_csv(SAMPLE_FILE)
    print("âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ!")
    print(f"  è®­ç»ƒé›†å¤§å°: {train_df.shape}")
except FileNotFoundError as e:
    print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥! é”™è¯¯: {e}")
    print(f"   è¯·å†æ¬¡ç¡®è®¤ä½ çš„ TRAIN_FILE è·¯å¾„æ˜¯å¦æ­£ç¡®: {TRAIN_FILE}")

# === 4: åŸºç¡€ç‰¹å¾å·¥ç¨‹ (æ‰€æœ‰æ¨¡å‹å…±ç”¨) [å·²ä¿®å¤] ===
print("\n--- æ­£åœ¨æ‰§è¡Œï¼šåŸºç¡€ç‰¹å¾å·¥ç¨‹ ---")

def create_base_features(df):
    df['text_a'] = df['prompt'] + " " + df['response_a']
    df['text_b'] = df['prompt'] + " " + df['response_b']
    df['combined_for_embedding'] = df['text_a'] + " [SEP] " + df['text_b']
    
    # é•¿åº¦ç‰¹å¾
    df["resp_a_len"] = df["response_a"].str.len()
    df["resp_b_len"] = df["response_b"].str.len()
    df["len_diff"] = df["resp_a_len"] - df["resp_b_len"]
    df["len_ratio"] = df["resp_a_len"] / (df["resp_b_len"] + 1e-6)
    
    # è¯æ±‡åº¦ç‰¹å¾
    df["lexical_a"] = df["response_a"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))
    df["lexical_b"] = df["response_b"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))
    df["lexical_diff"] = df["lexical_a"] - df["lexical_b"]
    return df

train_df = create_base_features(train_df)
test_df = create_base_features(test_df)

# --- ã€ä¿®å¤ Bugã€‘---
# 1. å…ˆåœ¨ DataFrame ä¸­åˆ›å»º 'label' åˆ—
train_df['label'] = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)
# 2. ç„¶å, å°† 'label' è¿™ä¸€åˆ— (Pandas Series) èµ‹å€¼ç»™ y_true_full
y_true_full = train_df['label']
# --- ã€ä¿®å¤å®Œæ¯•ã€‘---

print("âœ… åŸºç¡€ç‰¹å¾å·¥ç¨‹å®Œæˆ (å·²åˆ›å»º 3 ä¸ªåç½®ç‰¹å¾)ã€‚")

# === 5: åµŒå…¥ç”Ÿæˆ (MiniLM, E5) ä¸ ç›¸ä¼¼åº¦ç‰¹å¾ ===
print("\n--- æ­£åœ¨åŠ è½½ MiniLM æ¨¡å‹ ---")
model_minilm = SentenceTransformer(MINILM_PATH, device='cuda')

print("â³ æ­£åœ¨ä¸ºè®­ç»ƒé›†ç”Ÿæˆ MiniLM åµŒå…¥...")
train_emb_minilm = model_minilm.encode(
    train_df['combined_for_embedding'].tolist(), 
    show_progress_bar=True, batch_size=128, convert_to_numpy=True
)
print("â³ æ­£åœ¨ä¸ºæµ‹è¯•é›†ç”Ÿæˆ MiniLM åµŒå…¥...")
test_emb_minilm = model_minilm.encode(
    test_df['combined_for_embedding'].tolist(), 
    show_progress_bar=True, batch_size=128, convert_to_numpy=True
)

print("â³ æ­£åœ¨ç”Ÿæˆç›¸ä¼¼åº¦ç‰¹å¾ (Train)...")
resp_a_emb_train = model_minilm.encode(train_df['response_a'].tolist(), batch_size=128)
resp_b_emb_train = model_minilm.encode(train_df['response_b'].tolist(), batch_size=128)
print("â³ æ­£åœ¨ç”Ÿæˆç›¸ä¼¼åº¦ç‰¹å¾ (Test)...")
resp_a_emb_test = model_minilm.encode(test_df['response_a'].tolist(), batch_size=128)
resp_b_emb_test = model_minilm.encode(test_df['response_b'].tolist(), batch_size=128)

del model_minilm
clear_memory()
print("âœ… MiniLM æ¨¡å‹å·²é‡Šæ”¾")

print("â³ æ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦ç‰¹å¾...")
train_df['cosine_similarity'] = np.array([
    cosine_similarity(resp_a_emb_train[i].reshape(1, -1), resp_b_emb_train[i].reshape(1, -1))[0][0] 
    for i in range(len(resp_a_emb_train))
])
test_df['cosine_similarity'] = np.array([
    cosine_similarity(resp_a_emb_test[i].reshape(1, -1), resp_b_emb_test[i].reshape(1, -1))[0][0] 
    for i in range(len(resp_a_emb_test))
])

# --- E5 åµŒå…¥ (ç”¨äºæ¨¡å‹ C) ---
print("\n--- æ­£åœ¨åŠ è½½ E5 æ¨¡å‹ ---")
model_e5 = SentenceTransformer(E5_PATH, device='cuda')
print("â³ æ­£åœ¨ä¸ºè®­ç»ƒé›†ç”Ÿæˆ E5 åµŒå…¥...")
train_emb_e5 = model_e5.encode(
    train_df["combined_for_embedding"].tolist(), 
    batch_size=128, show_progress_bar=True, convert_to_numpy=True
)
print("â³ æ­£åœ¨ä¸ºæµ‹è¯•é›†ç”Ÿæˆ E5 åµŒå…¥...")
test_emb_e5 = model_e5.encode(
    test_df["combined_for_embedding"].tolist(), 
    batch_size=128, show_progress_bar=True, convert_to_numpy=True
)

del model_e5
clear_memory()
print("âœ… E5 æ¨¡å‹å·²é‡Šæ”¾")

# --- ä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ ---
print("â³ æ­£åœ¨ä¿å­˜æ‰€æœ‰åµŒå…¥å’Œç‰¹å¾åˆ° .npy æ–‡ä»¶...")
np.save(os.path.join(OUTPUT_DIR, 'train_emb_minilm.npy'), train_emb_minilm)
np.save(os.path.join(OUTPUT_DIR, 'test_emb_minilm.npy'), test_emb_minilm)
np.save(os.path.join(OUTPUT_DIR, 'train_emb_e5.npy'), train_emb_e5)
np.save(os.path.join(OUTPUT_DIR, 'test_emb_e5.npy'), test_emb_e5)

all_4_features_train = train_df[["len_diff", "len_ratio", "lexical_diff", "cosine_similarity"]].fillna(0).values
all_4_features_test = test_df[["len_diff", "len_ratio", "lexical_diff", "cosine_similarity"]].fillna(0).values
np.save(os.path.join(OUTPUT_DIR, 'train_features_4.npy'), all_4_features_train)
np.save(os.path.join(OUTPUT_DIR, 'test_features_4.npy'), all_4_features_test)

print("âœ… æ‰€æœ‰åµŒå…¥å’Œç‰¹å¾å·²ä¿å­˜ã€‚")

# === 7: å‡†å¤‡æœ€ç»ˆæ•°æ®é›† (ä½¿ç”¨åˆ†å±‚æŠ½æ ·) ===
print("\n--- æ­£åœ¨å‡†å¤‡æœ€ç»ˆæ•°æ®é›† ---")

# 1. åŠ è½½æ‰€æœ‰ç‰¹å¾
train_emb_minilm = np.load(os.path.join(OUTPUT_DIR, 'train_emb_minilm.npy'))
train_emb_e5 = np.load(os.path.join(OUTPUT_DIR, 'train_emb_e5.npy'))
all_4_features_train = np.load(os.path.join(OUTPUT_DIR, 'train_features_4.npy'))

# 2. å‡†å¤‡æ¨¡å‹ A å’Œ C çš„ç‰¹å¾é›†
X_A_full = np.hstack([train_emb_minilm, all_4_features_train])
X_C_full = np.hstack([train_emb_e5, all_4_features_train])

# 3. ä½¿ç”¨åˆ†å±‚æŠ½æ ·
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_indices, val_indices = next(sss.split(train_df, y_true_full))

# 4. å‡†å¤‡ LGBM æ¨¡å‹çš„è®­ç»ƒ/éªŒè¯é›†
X_train_A = X_A_full[train_indices]
X_val_A = X_A_full[val_indices]
X_train_C = X_C_full[train_indices]
X_val_C = X_C_full[val_indices]

y_train = y_true_full.iloc[train_indices]
y_val = y_true_full.iloc[val_indices]

# 5. å‡†å¤‡ LoRA æ¨¡å‹çš„è®­ç»ƒ/éªŒè¯é›†
train_df_lora = train_df.iloc[train_indices]
val_df_lora = train_df.iloc[val_indices]

print(f"âœ… åˆ†å±‚æŠ½æ ·æ•°æ®å‡†å¤‡å®Œæ¯•ã€‚")
print(f"   è®­ç»ƒé›†å¤§å°: {len(y_train)} | éªŒè¯é›†å¤§å°: {len(y_val)}")

# === 7: è®­ç»ƒå¹¶ä¿å­˜ [æ¨¡å‹ A (LGBM + MiniLM)] ===
print("\n--- æ­£åœ¨è®­ç»ƒ [æ¨¡å‹ A] ---")

lgbm_model_A = LGBMClassifier(
    n_estimators=300, learning_rate=0.05, num_leaves=64, random_state=42,
    device='gpu'
)

print("â³ æ­£åœ¨è®­ç»ƒ LGBM (MiniLM + 4 ç‰¹å¾)...")
lgbm_model_A.fit(X_train_A, y_train)

# è¯„ä¼°
val_preds_A = lgbm_model_A.predict_proba(X_val_A)
logloss_A = log_loss(y_val, val_preds_A)
print(f"ğŸ¯ [æ¨¡å‹ A] Validation LogLoss: {logloss_A:.5f}")

# ä¿å­˜æ¨¡å‹
lgbm_model_A.booster_.save_model(os.path.join(OUTPUT_DIR, 'model_A_lgbm.txt'))
print("âœ… [æ¨¡å‹ A] å·²ä¿å­˜ä¸º 'model_A_lgbm.txt'")

# === 8: è®­ç»ƒå¹¶ä¿å­˜ [æ¨¡å‹ C (LGBM + E5)] ===
print("\n--- æ­£åœ¨è®­ç»ƒ [æ¨¡å‹ C] ---")

lgbm_model_C = LGBMClassifier(
    n_estimators=300, learning_rate=0.05, num_leaves=64, random_state=42,
    device='gpu'
)

print("â³ æ­£åœ¨è®­ç»ƒ LGBM (E5 + 4 ç‰¹å¾)...")
lgbm_model_C.fit(X_train_C, y_train)

# è¯„ä¼°
val_preds_C = lgbm_model_C.predict_proba(X_val_C)
logloss_C = log_loss(y_val, val_preds_C)
print(f"ğŸ¯ [æ¨¡å‹ C] Validation LogLoss: {logloss_C:.5f}")

# ä¿å­˜æ¨¡å‹
lgbm_model_C.booster_.save_model(os.path.join(OUTPUT_DIR, 'model_C_lgbm.txt'))
print("âœ… [æ¨¡å‹ C] å·²ä¿å­˜ä¸º 'model_C_lgbm.txt'")

# === 9: è®­ç»ƒå¹¶ä¿å­˜ [æ¨¡å‹ B (LoRA DeBERTa-small)] [å·²ä¿®å¤ num_proc] ===
print("\n--- æ­£åœ¨è®­ç»ƒ [æ¨¡å‹ B] ---")

# 1. åŠ è½½æ¨¡å‹
local_model_path = "./deberta-small-local"
if not os.path.exists(local_model_path):
    shutil.copytree(DEBERTA_PATH, local_model_path)
tokenizer_B = AutoTokenizer.from_pretrained(local_model_path, local_files_only=True)
base_model_B = AutoModelForSequenceClassification.from_pretrained(local_model_path, num_labels=3, local_files_only=True)

# 2. é…ç½® LoRA
peft_config_B = LoraConfig(task_type=TaskType.SEQ_CLS, r=16, lora_alpha=32, lora_dropout=0.1, bias="none")
model_B = get_peft_model(base_model_B, peft_config_B)

# 3. æ•°æ®å¤„ç†
def preprocess_function_B(examples):
    texts = [f"é—®é¢˜: {p} [SEP] A: {a} [SEP] B: {b}" for p, a, b in zip(examples["prompt"], examples["response_a"], examples["response_b"])]
    return tokenizer_B(texts, truncation=True, padding="max_length", max_length=256)

train_dataset = Dataset.from_pandas(train_df_lora)
val_dataset = Dataset.from_pandas(val_df_lora)

# --- ã€æœ€ç»ˆä¿®å¤ã€‘å®Œå…¨åˆ é™¤ num_proc å‚æ•°ä»¥ç¦ç”¨å¤šè¿›ç¨‹ ---
print("â³ æ­£åœ¨ (å•è¿›ç¨‹) æ˜ å°„è®­ç»ƒé›† (DeBERTa)...")
tokenized_train_B = train_dataset.map(preprocess_function_B, batched=True) 
print("â³ æ­£åœ¨ (å•è¿›ç¨‹) æ˜ å°„éªŒè¯é›† (DeBERTa)...")
tokenized_val_B = val_dataset.map(preprocess_function_B, batched=True)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    return {"accuracy": accuracy_score(labels, np.argmax(logits, axis=-1))}

# 5. æ”¹è¿›çš„ LoRA è®­ç»ƒé…ç½®
training_args_B = TrainingArguments(
    output_dir="./ft_results_deberta",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    learning_rate=3e-4,
    save_total_limit=2,
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    dataloader_num_workers=0,  # ä¿æŒä¸º 0
    logging_steps=200,
    evaluation_strategy="epoch",
    fp16=True,
    fp16_full_eval=True,
    report_to=[]
)

trainer_B = Trainer(
    model=model_B,
    args=training_args_B,
    train_dataset=tokenized_train_B,
    eval_dataset=tokenized_val_B,
    tokenizer=tokenizer_B,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# 6. è®­ç»ƒ
print("â³ å¼€å§‹ LoRA å¾®è°ƒ (DeBERTa-small)...")
trainer_B.train()
print("âœ… å¾®è°ƒå®Œæˆã€‚")

# 7. è¯„ä¼°
val_logits_B = trainer_B.predict(tokenized_val_B).predictions
val_preds_B_uncalibrated = torch.softmax(torch.tensor(val_logits_B), dim=-1).numpy()
logloss_B_uncalibrated = log_loss(y_val, val_preds_B_uncalibrated)
print(f"ğŸ¯ [æ¨¡å‹ B] æ ¡å‡†å‰ Validation LogLoss: {logloss_B_uncalibrated:.5f}")

# 8. ä¿å­˜æœ€ä½³æ¨¡å‹
trainer_B.save_model(os.path.join(OUTPUT_DIR, 'model_B_deberta_lora'))
tokenizer_B.save_pretrained(os.path.join(OUTPUT_DIR, 'model_B_deberta_lora'))
print(f"âœ… [æ¨¡å‹ B] å·²ä¿å­˜åˆ° {os.path.join(OUTPUT_DIR, 'model_B_deberta_lora')}")

del model_B, base_model_B, trainer_B, tokenizer_B
clear_memory()

# === 10: è®­ç»ƒå¹¶ä¿å­˜ [æ¨¡å‹ D (LoRA RoBERTa-base)] [å·²ä¿®å¤ num_proc] ===
print("\n--- æ­£åœ¨è®­ç»ƒ [æ¨¡å‹ D] ---")

# 1. åŠ è½½æ¨¡å‹
local_model_path = "./roberta-base-local"
if not os.path.exists(local_model_path):
    shutil.copytree(ROBERTA_PATH, local_model_path)
tokenizer_D = AutoTokenizer.from_pretrained(local_model_path, local_files_only=True)
base_model_D = AutoModelForSequenceClassification.from_pretrained(local_model_path, num_labels=3, local_files_only=True)

# 2. é…ç½® LoRA
peft_config_D = LoraConfig(task_type=TaskType.SEQ_CLS, r=16, lora_alpha=32, lora_dropout=0.1, bias="none")
model_D = get_peft_model(base_model_D, peft_config_D)

# 3. æ•°æ®å¤„ç†
def preprocess_function_D(examples):
    texts = [f"é—®é¢˜: {p} [SEP] A: {a} [SEP] B: {b}" for p, a, b in zip(examples["prompt"], examples["response_a"], examples["response_b"])]
    return tokenizer_D(texts, truncation=True, padding="max_length", max_length=256)

train_dataset = Dataset.from_pandas(train_df_lora)
val_dataset = Dataset.from_pandas(val_df_lora)

# --- ã€æœ€ç»ˆä¿®å¤ã€‘å®Œå…¨åˆ é™¤ num_proc å‚æ•°ä»¥ç¦ç”¨å¤šè¿›ç¨‹ ---
print("â³ æ­£åœ¨ (å•è¿›ç¨‹) æ˜ å°„è®­ç»ƒé›† (RoBERTa)...")
tokenized_train_D = train_dataset.map(preprocess_function_D, batched=True)
print("â³ æ­£åœ¨ (å•è¿›ç¨‹) æ˜ å°„éªŒè¯é›† (RoBERTa)...")
tokenized_val_D = val_dataset.map(preprocess_function_D, batched=True)

# 4. è®­ç»ƒé…ç½®
training_args_D = TrainingArguments(
    output_dir="./ft_results_roberta_base",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    learning_rate=3e-4,
    save_total_limit=2,
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    dataloader_num_workers=0, # ä¿æŒä¸º 0
    logging_steps=200,
    evaluation_strategy="epoch",
    fp16=True,
    fp16_full_eval=True,
    report_to=[]
)

trainer_D = Trainer(
    model=model_D,
    args=training_args_D,
    train_dataset=tokenized_train_D,
    eval_dataset=tokenized_val_D,
    tokenizer=tokenizer_D,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# 5. è®­ç»ƒ
print("â³ å¼€å§‹ LoRA å¾®è°ƒ (RoBERTa-base)...")
trainer_D.train()
print("âœ… å¾®è°ƒå®Œæˆã€‚")

# 6. è¯„ä¼°
val_logits_D = trainer_D.predict(tokenized_val_D).predictions
val_preds_D_uncalibrated = torch.softmax(torch.tensor(val_logits_D), dim=-1).numpy()
logloss_D_uncalibrated = log_loss(y_val, val_preds_D_uncalibrated)
print(f"ğŸ¯ [æ¨¡å‹ D] æ ¡å‡†å‰ Validation LogLoss: {logloss_D_uncalibrated:.5f}")

# 7. ä¿å­˜æœ€ä½³æ¨¡å‹
trainer_D.save_model(os.path.join(OUTPUT_DIR, 'model_D_roberta_lora'))
tokenizer_D.save_pretrained(os.path.join(OUTPUT_DIR, 'model_D_roberta_lora'))
print(f"âœ… [æ¨¡å‹ D] å·²ä¿å­˜åˆ° {os.path.join(OUTPUT_DIR, 'model_D_roberta_lora')}")

del model_D, base_model_D, trainer_D, tokenizer_D
clear_memory()

# === 11: æ¦‚ç‡æ ¡å‡†ä¸æƒé‡ä¼˜åŒ– ===
print("\n--- æ­£åœ¨æ‰§è¡Œ: æ¦‚ç‡æ ¡å‡†ä¸æƒé‡ä¼˜åŒ– ---")

def temperature_scale(logits, T):
    logits_T = logits / T
    return torch.softmax(torch.tensor(logits_T), dim=-1).numpy()

def loss_fn_cal(T, logits, labels):
    probs_T = temperature_scale(logits, T)
    return log_loss(labels, probs_T)

# --- æ ¡å‡†æ¨¡å‹ B (DeBERTa) ---
res_B = minimize(loss_fn_cal, x0=[1.0], args=(val_logits_B, y_val), 
               bounds=[(0.5, 5.0)], method="L-BFGS-B")
T_opt_B = res_B.x[0]
logloss_B_calibrated = res_B.fun
print(f"ğŸ“ [æ¨¡å‹ B] DeBERTa T = {T_opt_B:.3f} | æ ¡å‡†å Loss: {logloss_B_calibrated:.5f}")
val_preds_B = temperature_scale(val_logits_B, T_opt_B)
np.save(os.path.join(OUTPUT_DIR, 'temp_B.npy'), np.array([T_opt_B]))

# --- æ ¡å‡†æ¨¡å‹ D (RoBERTa) ---
res_D = minimize(loss_fn_cal, x0=[1.0], args=(val_logits_D, y_val), 
               bounds=[(0.5, 5.0)], method="L-BFGS-B")
T_opt_D = res_D.x[0]
logloss_D_calibrated = res_D.fun
print(f"ğŸ“ [æ¨¡å‹ D] RoBERTa T = {T_opt_D:.3f} | æ ¡å‡†å Loss: {logloss_D_calibrated:.5f}")
val_preds_D = temperature_scale(val_logits_D, T_opt_D)
np.save(os.path.join(OUTPUT_DIR, 'temp_D.npy'), np.array([T_opt_D]))

# --- ä¼˜åŒ–é›†æˆæƒé‡ (é‡‡çº³ SLSQP å»ºè®®) ---
def loss_fn_ensemble(weights):
    wA, wB, wC = weights
    wD = 1.0 - wA - wB - wC
    if wD < 0 or min(weights) < 0: return 100.0
    ensemble_val_preds = (
        (val_preds_A * wA) + (val_preds_B * wB) +
        (val_preds_C * wC) + (val_preds_D * wD)
    )
    ensemble_val_preds = np.clip(ensemble_val_preds, 1e-7, 1 - 1e-7)
    return log_loss(y_val, ensemble_val_preds)

initial_weights = [0.4, 0.1, 0.4]  # [wA, wB, wC]
bounds = [(0, 1), (0, 1), (0, 1)]
constraints = {'type': 'ineq', 'fun': lambda w: 1.0 - sum(w)}

res = minimize(
    loss_fn_ensemble, initial_weights, method='SLSQP',
    bounds=bounds, constraints=constraints
)

wA_opt, wB_opt, wC_opt = res.x
wD_opt = 1.0 - sum(res.x)
print(f"\nğŸ¯ æœ€ä½³é›†æˆéªŒè¯ LogLoss: {res.fun:.5f}")
print("--- æœ€ä½³æƒé‡ ---")
print(f"æ¨¡å‹ A (LGBM+MiniLM): {wA_opt:.4f}")
print(f"æ¨¡å‹ B (LoRA-DeBERTa): {wB_opt:.4f}")
print(f"æ¨¡å‹ C (LGBM+E5):     {wC_opt:.4f}")
print(f"æ¨¡å‹ D (LoRA-RoBERTa): {wD_opt:.4f}")

# --- ä¿å­˜æœ€ç»ˆæƒé‡ ---
final_weights = np.array([wA_opt, wB_opt, wC_opt, wD_opt])
np.save(os.path.join(OUTPUT_DIR, 'ensemble_weights.npy'), final_weights)
print(f"âœ… æœ€ç»ˆæƒé‡å·²ä¿å­˜åˆ° 'ensemble_weights.npy'")

# === 12: è®­ç»ƒå®Œæˆ ===
print(f"ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–å®Œæ¯•ï¼ğŸ‰ğŸ‰ğŸ‰")
print(f"æ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶éƒ½å·²ä¿å­˜åœ¨ä½ çš„è¾“å‡ºæ–‡ä»¶å¤¹ä¸­: \n{OUTPUT_DIR}")

print("\nä½ éœ€è¦æ‰“åŒ…å¹¶ä¸Šä¼ åˆ° Kaggle Dataset çš„æ–‡ä»¶ï¼š")
print("---------------------------------")
print("1. model_baseline.joblib")
print("2. vectorizer_baseline.joblib")
print("3. scaler_baseline.joblib")
print("4. model_A_lgbm.txt")
print("5. model_C_lgbm.txt")
print("6. model_B_deberta_lora/ (æ•´ä¸ªæ–‡ä»¶å¤¹)")
print("7. model_D_roberta_lora/ (æ•´ä¸ªæ–‡ä»¶å¤¹)")
print("8. temp_B.npy")
print("9. temp_D.npy")
print("10. ensemble_weights.npy")

# === æ­¥éª¤ 14: é«˜çº§é”™è¯¯åˆ†æ (A/C vs D) ===
print("\n--- æ­£åœ¨æ‰§è¡Œï¼šé«˜çº§é”™è¯¯åˆ†æ (A/C å¤±è´¥ vs D æˆåŠŸ) ---")

# 1. è·å–æ‰€æœ‰æ¨¡å‹çš„éªŒè¯é›†é¢„æµ‹ç±»åˆ«
pred_class_A = np.argmax(val_preds_A, axis=1) # (æ¥è‡ªå•å…ƒæ ¼ 8)
pred_class_C = np.argmax(val_preds_C, axis=1) # (æ¥è‡ªå•å…ƒæ ¼ 9)
pred_class_D = np.argmax(val_preds_D, axis=1) # (æ¥è‡ªå•å…ƒæ ¼ 12)
# y_val æ˜¯çœŸå®æ ‡ç­¾ (æ¥è‡ªå•å…ƒæ ¼ 7)

# 2. æ‰¾åˆ°æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ ·æœ¬ç´¢å¼•
#    (A é”™äº† AND C é”™äº† AND D å¯¹äº†)
error_indices = np.where(
    (pred_class_A != y_val) &
    (pred_class_C != y_val) &
    (pred_class_D == y_val)
)[0]

print(f"âœ… æ‰¾åˆ° {len(error_indices)} ä¸ªæ ·æœ¬ï¼Œå…¶ä¸­æ¨¡å‹ A å’Œ C éƒ½å¤±è´¥äº†ï¼Œä½†æ¨¡å‹ D æˆåŠŸäº†ã€‚")

# 3. æå–è¿™äº›æ ·æœ¬çš„åŸå§‹æ–‡æœ¬ (ä½¿ç”¨ 'val_indices')
analysis_df = train_df.loc[val_indices[error_indices]].copy()
analysis_df['true_label'] = y_val.iloc[error_indices]
analysis_df['pred_D_label'] = pred_class_D[error_indices]

print("\n--- æ­£åœ¨æ˜¾ç¤º A/C çš„å…±åŒç›²åŒº (å‰ 10 ä¸ªæ ·æœ¬) ---")

for idx, row in analysis_df.head(10).iterrows():
    print(f"\n--- æ ·æœ¬ ID: {idx} | çœŸå®æ ‡ç­¾: {row['true_label']} (æ¨¡å‹ D çŒœå¯¹äº†) ---")
    
    # è·å– A å’Œ C çš„é”™è¯¯é¢„æµ‹
    idx_in_val_set = np.where(val_indices == idx)[0][0]
    print(f"    æ¨¡å‹ A çš„é”™è¯¯é¢„æµ‹: {pred_class_A[idx_in_val_set]} (ç½®ä¿¡åº¦: {val_preds_A[idx_in_val_set].max():.2%})")
    print(f"    æ¨¡å‹ C çš„é”™è¯¯é¢„æµ‹: {pred_class_C[idx_in_val_set]} (ç½®ä¿¡åº¦: {val_preds_C[idx_in_val_set].max():.2%})")
    
    print(f"    Prompt: {row['prompt'][:100]}...")
    print(f"    Response A: {row['response_a'][:100]}...")
    print(f"    Response B: {row['response_b'][:100]}...")

# === æ­¥éª¤ 15: æ¨¡å‹ D (RoBERTa-base) çš„â€œå®šé‡â€é”™è¯¯åˆ†æ ===

print("--- æ­£åœ¨ä¸ºæ¨¡å‹ D (RoBERTa-base) ç”Ÿæˆæ··æ·†çŸ©é˜µ ---")

# (y_val æ˜¯çœŸå®æ ‡ç­¾)
# (val_preds_D æ˜¯æ¨¡å‹ D æ ¡å‡†åçš„éªŒè¯é›†é¢„æµ‹)
y_pred_classes_D = np.argmax(val_preds_D, axis=1)

cm_D = confusion_matrix(y_val, y_pred_classes_D)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_D, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['A Win', 'B Win', 'Tie'], 
            yticklabels=['A Win', 'B Win', 'Tie'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Model D (LoRA RoBERTa-base @ 1.06579)')
plt.savefig("confusion_matrix_model_D.png") # ä¿å­˜ä¸ºæ–°æ–‡ä»¶å
print("âœ… æ¨¡å‹ D çš„æ··æ·†çŸ©é˜µå·²ä¿å­˜ä¸º 'confusion_matrix_model_D.png'")

# === æ­¥éª¤ 16: æ¨¡å‹ D (RoBERTa-base) çš„â€œå®šæ€§â€é”™è¯¯åˆ†æ ===

print("--- æ­£åœ¨ä¸ºæ¨¡å‹ D (RoBERTa-base) æŸ¥æ‰¾æœ€å·®çš„ 10 ä¸ªé¢„æµ‹ ---")

# 1. (y_val æ˜¯çœŸå®æ ‡ç­¾, val_preds_D æ˜¯é¢„æµ‹æ¦‚ç‡)
# 2. (val_indices æ˜¯éªŒè¯é›†åœ¨ train_df ä¸­çš„ç´¢å¼•)
val_df_D = train_df.loc[val_indices].copy()
val_df_D['true_label'] = y_val
val_df_D['pred_prob_A'] = val_preds_D[:, 0]
val_df_D['pred_prob_B'] = val_preds_D[:, 1]
val_df_D['pred_prob_Tie'] = val_preds_D[:, 2]

# 3. æ‰¾å‡ºæ¨¡å‹é¢„æµ‹çš„ç±»åˆ«
val_df_D['predicted_label'] = y_pred_classes_D

# 4. æ‰¾å‡ºæ‰€æœ‰é¢„æµ‹é”™è¯¯çš„æ ·æœ¬
error_df_D = val_df_D[val_df_D['true_label'] != val_df_D['predicted_label']].copy()

# 5. æ‰¾å‡ºé”™è¯¯æ ·æœ¬ä¸­ï¼Œæ¨¡å‹å¯¹â€œé”™è¯¯ç­”æ¡ˆâ€çš„ç½®ä¿¡åº¦
error_df_D['confidence_in_wrong_answer'] = np.max(val_preds_D[val_df_D.index.isin(error_df_D.index)], axis=1)

# 6. æŒ‰â€œå¯¹é”™è¯¯ç­”æ¡ˆçš„ç½®ä¿¡åº¦â€é™åºæ’åˆ—ï¼Œæ‰¾å‡ºæœ€è‡ªä¿¡çš„ 10 ä¸ªé”™è¯¯
worst_misses_D = error_df_D.sort_values(by='confidence_in_wrong_answer', ascending=False).head(10)

print("--- 10ä¸ªæ¨¡å‹ Dâ€œæœ€è‡ªä¿¡çš„é”™è¯¯â€ (ç”¨äºæŠ¥å‘Šåˆ†æ) ---")
# æ‰“å°è¿™äº›æ ·æœ¬çš„å…³é”®ä¿¡æ¯
for idx, row in worst_misses_D.iterrows():
    print(f"\n--- æ ·æœ¬ ID: {idx} | çœŸå®æ ‡ç­¾: {row['true_label']} | é”™è¯¯é¢„æµ‹: {row['predicted_label']} ---")
    print(f"    æ¨¡å‹å¯¹(é”™è¯¯çš„)é¢„æµ‹ {row['predicted_label']} çš„ç½®ä¿¡åº¦: {row['confidence_in_wrong_answer']:.2%}")
    print(f"    (A/B/Tie æ¦‚ç‡): {row['pred_prob_A']:.2f} / {row['pred_prob_B']:.2f} / {row['pred_prob_Tie']:.2f}")
    print(f"    Prompt: {row['prompt'][:100]}...")
    print(f"    Response A: {row['response_a'][:100]}...")
    print(f"    Response B: {row['response_b'][:100]}...")

worst_misses_D.to_csv("worst_misses_model_D.csv", index=False)
print("\nâœ… æ¨¡å‹ D æœ€å·®é¢„æµ‹çš„è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ° 'worst_misses_model_D.csv'")

# === æ­¥éª¤ 15: ä¼˜åŒ–ç‰ˆæ¨¡å‹ A (LGBM+MiniLM) çš„æ··æ·†çŸ©é˜µ ===
print("--- æ­£åœ¨ä¸ºæ¨¡å‹ A (LGBM+MiniLM) ç”Ÿæˆæ··æ·†çŸ©é˜µ ---")

# (y_val æ˜¯çœŸå®æ ‡ç­¾)
# (val_preds_A æ˜¯æ¨¡å‹ A çš„éªŒè¯é›†é¢„æµ‹)
y_pred_classes_A = np.argmax(val_preds_A, axis=1)

cm_A = confusion_matrix(y_val, y_pred_classes_A)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_A, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['A Win', 'B Win', 'Tie'], 
            yticklabels=['A Win', 'B Win', 'Tie'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Optimized LGBM (MiniLM + 4 Feat @ 1.03534)')
plt.savefig(os.path.join(OUTPUT_DIR, "confusion_matrix_model_A.png"))
print(f"âœ… æ¨¡å‹ A çš„æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {os.path.join(OUTPUT_DIR, 'confusion_matrix_model_A.png')}")

# === æ­¥éª¤ 16: ä¼˜åŒ–ç‰ˆæ¨¡å‹ C (LGBM+E5) çš„æ··æ·†çŸ©é˜µ ===
print("--- æ­£åœ¨ä¸ºæ¨¡å‹ C (LGBM+E5) ç”Ÿæˆæ··æ·†çŸ©é˜µ ---")

# (y_val æ˜¯çœŸå®æ ‡ç­¾)
# (val_preds_C æ˜¯æ¨¡å‹ C çš„éªŒè¯é›†é¢„æµ‹)
y_pred_classes_C = np.argmax(val_preds_C, axis=1)

cm_C = confusion_matrix(y_val, y_pred_classes_C)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_C, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['A Win', 'B Win', 'Tie'], 
            yticklabels=['A Win', 'B Win', 'Tie'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Optimized LGBM (E5 + 4 Feat @ 1.03605)')
plt.savefig(os.path.join(OUTPUT_DIR, "confusion_matrix_model_C.png"))
print(f"âœ… æ¨¡å‹ C çš„æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {os.path.join(OUTPUT_DIR, 'confusion_matrix_model_C.png')}")

# === æ­¥éª¤ 17: (ä¿®æ­£ç‰ˆ) è®­ç»ƒ [æ¨¡å‹ E (LoRA DeBERTa-v3-base)] ===
print("\n--- æ­£åœ¨è®­ç»ƒ [æ¨¡å‹ E (LoRA DeBERTa-v3-base)] ---")

# 1. å®šä¹‰æ–°æ¨¡å‹è·¯å¾„
DEBERTAv3_BASE_PATH = os.path.join(MODELS_DIR, "deberta-v3-base")

if not os.path.exists(DEBERTAv3_BASE_PATH):
    print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° DeBERTa-v3-base æ¨¡å‹ã€‚")
    print(f"   è¯·ç¡®ä¿å®ƒå­˜åœ¨äº: {DEBERTAv3_BASE_PATH}")
else:
    print(f"âœ… æ‰¾åˆ° DeBERTa-v3-base æ¨¡å‹: {DEBERTAv3_BASE_PATH}")

    # 2. åŠ è½½æ¨¡å‹ (ä½¿ç”¨æ–°å˜é‡å)
    local_model_path_E = "./deberta-v3-base-local"
    if not os.path.exists(local_model_path_E):
        shutil.copytree(DEBERTAv3_BASE_PATH, local_model_path_E)
        
    tokenizer_E = AutoTokenizer.from_pretrained(local_model_path_E, local_files_only=True)
    base_model_E = AutoModelForSequenceClassification.from_pretrained(local_model_path_E, num_labels=3, local_files_only=True)

    # 3. é…ç½® LoRA
    # ã€ä¿®å¤ã€‘: å°† Task_TYPE ä¿®æ­£ä¸º TaskType
    peft_config_E = LoraConfig(task_type=TaskType.SEQ_CLS, r=16, lora_alpha=32, lora_dropout=0.1, bias="none")
    model_E = get_peft_model(base_model_E, peft_config_E)

    # 4. æ•°æ®å¤„ç† (ä½¿ç”¨ä¸æ¨¡å‹ D ç›¸åŒçš„å¤„ç†å‡½æ•°)
    def preprocess_function_E(examples):
        texts = [f"é—®é¢˜: {p} [SEP] A: {a} [SEP] B: {b}" for p, a, b in zip(examples["prompt"], examples["response_a"], examples["response_b"])]
        return tokenizer_E(texts, truncation=True, padding="max_length", max_length=256)

    # (train_df_lora å’Œ val_df_lora æ¥è‡ª æ­¥éª¤ 7)
    train_dataset = Dataset.from_pandas(train_df_lora)
    val_dataset = Dataset.from_pandas(val_df_lora)

    print("â³ æ­£åœ¨ (å•è¿›ç¨‹) æ˜ å°„è®­ç»ƒé›† (DeBERTa-v3-base)...")
    tokenized_train_E = train_dataset.map(preprocess_function_E, batched=True)
    print("â³ æ­£åœ¨ (å•è¿›ç¨‹) æ˜ å°„éªŒè¯é›† (DeBERTa-v3-base)...")
    tokenized_val_E = val_dataset.map(preprocess_function_E, batched=True)

    # 5. è®­ç»ƒé…ç½® (ä½¿ç”¨åŸå§‹ 3e-4 å­¦ä¹ ç‡)
    training_args_E = TrainingArguments(
        output_dir="./ft_results_deberta_base", # æ–°çš„è¾“å‡ºç›®å½•
        per_device_train_batch_size=8,
        per_device_eval_batch_size=16,
        gradient_accumulation_steps=2,
        num_train_epochs=3,
        learning_rate=3e-4, # (ä½¿ç”¨åŸå§‹å­¦ä¹ ç‡)
        save_total_limit=2,
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        dataloader_num_workers=0, 
        logging_steps=200,
        evaluation_strategy="epoch",
        fp16=True,
        fp16_full_eval=True,
        report_to=[]
    )
    
    # (compute_metrics æ¥è‡ª æ­¥éª¤ 9)
    trainer_E = Trainer(
        model=model_E,
        args=training_args_E,
        train_dataset=tokenized_train_E,
        eval_dataset=tokenized_val_E,
        tokenizer=tokenizer_E,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )

    # 6. è®­ç»ƒ
    print("â³ å¼€å§‹ LoRA å¾®è°ƒ (DeBERTa-v3-base)...")
    trainer_E.train()
    print("âœ… å¾®è°ƒå®Œæˆã€‚")

    # 7. è¯„ä¼° (åˆ›å»ºæ–°å˜é‡)
    val_logits_E = trainer_E.predict(tokenized_val_E).predictions
    val_preds_E_uncalibrated = torch.softmax(torch.tensor(val_logits_E), dim=-1).numpy()
    logloss_E_uncalibrated = log_loss(y_val, val_preds_E_uncalibrated)
    print(f"ğŸ¯ [æ¨¡å‹ E] æ ¡å‡†å‰ Validation LogLoss: {logloss_E_uncalibrated:.5f}")

    # 8. ä¿å­˜æœ€ä½³æ¨¡å‹ (ä¿å­˜åˆ°æ–°è·¯å¾„)
    new_model_path_E = os.path.join(OUTPUT_DIR, 'model_E_deberta_base_lora')
    trainer_E.save_model(new_model_path_E)
    tokenizer_E.save_pretrained(new_model_path_E)
    print(f"âœ… [æ¨¡å‹ E] å·²ä¿å­˜åˆ° {new_model_path_E}")

    del model_E, base_model_E, trainer_E, tokenizer_E
    clear_memory()

# === æ­¥éª¤ 18: (æ–°) 5 æ¨¡å‹æ ¡å‡†ä¸æƒé‡ä¼˜åŒ– (A+B+C+D_orig+E_new) ===
print("\n--- æ­£åœ¨æ‰§è¡Œ: 5 æ¨¡å‹æ ¡å‡†ä¸æƒé‡ä¼˜åŒ– ---")

# (é‡ç”¨ æ­¥éª¤ 11 çš„å‡½æ•°)
def temperature_scale(logits, T):
    logits_T = logits / T
    return torch.softmax(torch.tensor(logits_T), dim=-1).numpy()

def loss_fn_cal(T, logits, labels):
    probs_T = temperature_scale(logits, T)
    return log_loss(labels, probs_T)

# --- 1. æ ¡å‡† LoRA æ¨¡å‹ B, D, E ---
# B (æ¥è‡ªæ­¥éª¤ 9)
res_B = minimize(loss_fn_cal, x0=[1.0], args=(val_logits_B, y_val), bounds=[(0.5, 5.0)], method="L-BFGS-B")
T_opt_B = res_B.x[0]
val_preds_B = temperature_scale(val_logits_B, T_opt_B)
print(f"ğŸ“ [æ¨¡å‹ B] DeBERTa-small T = {T_opt_B:.3f} | Loss: {res_B.fun:.5f}")
# (temp_B.npy å·²åœ¨ æ­¥éª¤ 11 ä¿å­˜)

# D (åŸå§‹, æ¥è‡ªæ­¥éª¤ 10)
res_D = minimize(loss_fn_cal, x0=[1.0], args=(val_logits_D, y_val), bounds=[(0.5, 5.0)], method="L-BFGS-B")
T_opt_D = res_D.x[0]
val_preds_D = temperature_scale(val_logits_D, T_opt_D)
print(f"ğŸ“ [æ¨¡å‹ D-Orig] RoBERTa-base T = {T_opt_D:.3f} | Loss: {res_D.fun:.5f}")
# (temp_D.npy å·²åœ¨ æ­¥éª¤ 11 ä¿å­˜)

# E (æ–°, æ¥è‡ªæ­¥éª¤ 17)
res_E = minimize(loss_fn_cal, x0=[1.0], args=(val_logits_E, y_val), bounds=[(0.5, 5.0)], method="L-BFGS-B")
T_opt_E = res_E.x[0]
logloss_E_calibrated = res_E.fun
val_preds_E = temperature_scale(val_logits_E, T_opt_E)
print(f"ğŸ“ [æ¨¡å‹ E-New] DeBERTa-base T = {T_opt_E:.3f} | Loss: {logloss_E_calibrated:.5f}")
# ä¿å­˜æ–°çš„æ¸©åº¦æ–‡ä»¶
np.save(os.path.join(OUTPUT_DIR, 'temp_E.npy'), np.array([T_opt_E]))


# --- 2. ä¼˜åŒ– 5 æ¨¡å‹é›†æˆæƒé‡ ---
# (val_preds_A æ¥è‡ªæ­¥éª¤ 8, val_preds_C æ¥è‡ªæ­¥éª¤ 9)

def loss_fn_ensemble_5(weights):
    wA, wB, wC, wD = weights
    wE = 1.0 - wA - wB - wC - wD
    if wE < 0 or min(weights) < 0: return 100.0
    ensemble_val_preds = (
        (val_preds_A * wA) + (val_preds_B * wB) +
        (val_preds_C * wC) + (val_preds_D * wD) + 
        (val_preds_E * wE) # æ·»åŠ æ¨¡å‹ E
    )
    ensemble_val_preds = np.clip(ensemble_val_preds, 1e-7, 1 - 1e-7)
    return log_loss(y_val, ensemble_val_preds)

initial_weights_5 = [0.3, 0.1, 0.3, 0.1]  # [wA, wB, wC, wD]
bounds_5 = [(0, 1), (0, 1), (0, 1), (0, 1)]
constraints_5 = {'type': 'ineq', 'fun': lambda w: 1.0 - sum(w)}

res_5 = minimize(
    loss_fn_ensemble_5, initial_weights_5, method='SLSQP',
    bounds=bounds_5, constraints=constraints_5
)

wA_opt_5, wB_opt_5, wC_opt_5, wD_opt_5 = res_5.x
wE_opt_5 = 1.0 - sum(res_5.x)
print(f"\nğŸ¯ [5 æ¨¡å‹é›†æˆ] æœ€ä½³é›†æˆéªŒè¯ LogLoss: {res_5.fun:.5f}")
print("--- [5 æ¨¡å‹] æœ€ä½³æƒé‡ ---")
print(f"æ¨¡å‹ A (LGBM+MiniLM): {wA_opt_5:.4f}")
print(f"æ¨¡å‹ B (LoRA-DeBERTa-small): {wB_opt_5:.4f}")
print(f"æ¨¡å‹ C (LGBM+E5):     {wC_opt_5:.4f}")
print(f"æ¨¡å‹ D (LoRA-RoBERTa-Orig): {wD_opt_5:.4f}")
print(f"æ¨¡å‹ E (LoRA-DeBERTa-base): {wE_opt_5:.4f}")

# --- 3. ä¿å­˜æœ€ç»ˆçš„ 5 æ¨¡å‹æƒé‡ ---
final_weights_5 = np.array([wA_opt_5, wB_opt_5, wC_opt_5, wD_opt_5, wE_opt_5])
np.save(os.path.join(OUTPUT_DIR, 'ensemble_weights_5model.npy'), final_weights_5)
print(f"âœ… æœ€ç»ˆ(5æ¨¡å‹)æƒé‡å·²ä¿å­˜åˆ° 'ensemble_weights_5model.npy'")

# === æ­¥éª¤ 19: (æ–°) æ¨¡å‹ E (DeBERTa-v3-base) çš„â€œå®šé‡â€é”™è¯¯åˆ†æ ===
print("\n--- æ­£åœ¨ä¸ºæ¨¡å‹ E (DeBERTa-v3-base) ç”Ÿæˆæ··æ·†çŸ©é˜µ ---")

# (y_val æ˜¯çœŸå®æ ‡ç­¾)
# (val_preds_E æ˜¯æ¥è‡ª æ­¥éª¤ 18 çš„ E æ ¡å‡†åé¢„æµ‹)
y_pred_classes_E = np.argmax(val_preds_E, axis=1)

cm_E = confusion_matrix(y_val, y_pred_classes_E)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_E, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['A Win', 'B Win', 'Tie'], 
            yticklabels=['A Win', 'B Win', 'Tie'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title(f'Confusion Matrix for Model E (LoRA DeBERTa-base @ {logloss_E_calibrated:.5f})')
plt.savefig(os.path.join(OUTPUT_DIR, "confusion_matrix_model_E.png"))
print(f"âœ… æ¨¡å‹ E çš„æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {os.path.join(OUTPUT_DIR, 'confusion_matrix_model_E.png')}")

# === æ­¥éª¤ 21: (æ–°) ä¸º LGBM åˆ›å»º N-gram å·®å¼‚ç‰¹å¾ ===
print("\n--- æ­£åœ¨ä¸º LGBM åˆ›å»º N-gram ç‰¹å¾ ---")

# 1. åˆ›å»ºä¸€ä¸ªåŒ…å«è®­ç»ƒé›†ä¸­æ‰€æœ‰ response_a å’Œ response_b çš„è¯­æ–™åº“
print("â³ æ­£åœ¨æ„å»º N-gram è¯­æ–™åº“...")
corpus = pd.concat([
    train_df['response_a'],
    train_df['response_b']
]).astype(str).unique() # ä½¿ç”¨ unique å‡å°‘ fit çš„å·¥ä½œé‡

# 2. åˆå§‹åŒ–å¹¶è®­ç»ƒ CountVectorizer
# æˆ‘ä»¬ä½¿ç”¨ 2000 ä¸ªç‰¹å¾, åŒ…å«å•è¯(1,1)å’ŒåŒè¯(2,2)
vectorizer = CountVectorizer(
    max_features=2000,
    ngram_range=(1, 2), # åŒ…å« 1-grams å’Œ 2-grams
    stop_words='english',
    dtype=np.float32 # èŠ‚çœå†…å­˜
)

print("â³ æ­£åœ¨è®­ç»ƒ CountVectorizer (fit)...")
vectorizer.fit(corpus)

# 3. è½¬æ¢è®­ç»ƒé›†å’Œæµ‹è¯•é›†
print("â³ æ­£åœ¨è½¬æ¢ (transform) è®­ç»ƒé›† A/B...")
train_ngram_a = vectorizer.transform(train_df['response_a'].astype(str))
train_ngram_b = vectorizer.transform(train_df['response_b'].astype(str))

print("â³ æ­£åœ¨è½¬æ¢ (transform) æµ‹è¯•é›† A/B...")
test_ngram_a = vectorizer.transform(test_df['response_a'].astype(str))
test_ngram_b = vectorizer.transform(test_df['response_b'].astype(str))

# 4. åˆ›å»ºå·®å¼‚ç‰¹å¾ (A - B)
# è¿™ä¼šåˆ›å»ºä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œå…¶ä¸­åŒ…å« A ä¸­ç‹¬æœ‰/æ›´å¤šçš„ N-gramï¼ˆæ­£å€¼ï¼‰å’Œ B ä¸­ç‹¬æœ‰/æ›´å¤šçš„ N-gramï¼ˆè´Ÿå€¼ï¼‰
train_ngram_diff = (train_ngram_a - train_ngram_b)
test_ngram_diff = (test_ngram_a - test_ngram_b)

print(f"âœ… N-gram å·®å¼‚ç‰¹å¾å·²åˆ›å»º (å½¢çŠ¶: {train_ngram_diff.shape})")

# 5. ä¿å­˜ç‰¹å¾å’Œ vectorizer åˆ° output
from scipy.sparse import save_npz, load_npz
import joblib

print("â³ æ­£åœ¨ä¿å­˜ N-gram ç‰¹å¾å’Œ Vectorizer...")
save_npz(os.path.join(OUTPUT_DIR, 'train_ngram_diff.npz'), train_ngram_diff)
save_npz(os.path.join(OUTPUT_DIR, 'test_ngram_diff.npz'), test_ngram_diff)
joblib.dump(vectorizer, os.path.join(OUTPUT_DIR, 'vectorizer_ngram.joblib'))

print("âœ… N-gram æ¨¡å—å·²ä¿å­˜ã€‚")

del corpus, train_ngram_a, train_ngram_b, test_ngram_a, test_ngram_b
clear_memory()

# === æ­¥éª¤ 22: (æ–°) é‡æ–°è®­ç»ƒ [æ¨¡å‹ A-Ngram (LGBM + MiniLM + Ngram)] ===
print("\n--- æ­£åœ¨è®­ç»ƒ [æ¨¡å‹ A-Ngram] ---")
from scipy.sparse import hstack, load_npz

# 1. åŠ è½½æ‰€æœ‰éœ€è¦çš„ç‰¹å¾
# (æ¥è‡ª æ­¥éª¤ 7)
train_emb_minilm = np.load(os.path.join(OUTPUT_DIR, 'train_emb_minilm.npy'))
all_4_features_train = np.load(os.path.join(OUTPUT_DIR, 'train_features_4.npy'))
# (æ¥è‡ª æ­¥éª¤ 21)
train_ngram_diff = load_npz(os.path.join(OUTPUT_DIR, 'train_ngram_diff.npz'))

print(f"  MiniLM åµŒå…¥: {train_emb_minilm.shape}")
print(f"  4 ä¸ªåç½®ç‰¹å¾: {all_4_features_train.shape}")
print(f"  N-gram å·®å¼‚: {train_ngram_diff.shape}")

# 2. å †å æ‰€æœ‰ç‰¹å¾
# (hstack ä¼šè‡ªåŠ¨å¤„ç†ç¨€ç–å’Œå¯†é›†çŸ©é˜µçš„å †å )
X_A_ngram_full = hstack([
    train_emb_minilm,
    all_4_features_train,
    train_ngram_diff
]).tocsr() # è½¬æ¢ä¸º CSR æ ¼å¼ä»¥ä¾¿äºç´¢å¼•

print(f"âœ… æ–°çš„ A ç‰¹å¾çŸ©é˜µå·²åˆ›å»º (å½¢çŠ¶: {X_A_ngram_full.shape})")

# 3. ä½¿ç”¨ç›¸åŒçš„åˆ†å±‚æŠ½æ ·ç´¢å¼• (æ¥è‡ª æ­¥éª¤ 7)
X_train_A_ngram = X_A_ngram_full[train_indices]
X_val_A_ngram = X_A_ngram_full[val_indices]
# (y_train å’Œ y_val æ¥è‡ª æ­¥éª¤ 7)

# 4. è®­ç»ƒæ–°çš„ LGBM æ¨¡å‹
lgbm_model_A_ngram = LGBMClassifier(
    n_estimators=300, 
    learning_rate=0.05, 
    num_leaves=64, 
    random_state=42,
    device='gpu'
)

print("â³ æ­£åœ¨è®­ç»ƒ LGBM (MiniLM + 4 ç‰¹å¾ + N-gram)...")
lgbm_model_A_ngram.fit(X_train_A_ngram, y_train)

# 5. è¯„ä¼° (åˆ›å»ºæ–°å˜é‡)
val_preds_A_ngram = lgbm_model_A_ngram.predict_proba(X_val_A_ngram)
logloss_A_ngram = log_loss(y_val, val_preds_A_ngram)
print(f"ğŸ¯ [æ¨¡å‹ A-Ngram] Validation LogLoss: {logloss_A_ngram:.5f}")
print(f"  (åŸå§‹æ¨¡å‹ A LogLoss: {logloss_A:.5f})") # logloss_A æ¥è‡ª æ­¥éª¤ 8

# 6. ä¿å­˜æ–°æ¨¡å‹
lgbm_model_A_ngram.booster_.save_model(os.path.join(OUTPUT_DIR, 'model_A_lgbm_ngram.txt'))
print("âœ… [æ¨¡å‹ A-Ngram] å·²ä¿å­˜ä¸º 'model_A_lgbm_ngram.txt'")

del X_A_ngram_full, X_train_A_ngram, X_val_A_ngram, lgbm_model_A_ngram
clear_memory()

# === æ­¥éª¤ 23: (æ–°) é‡æ–°è®­ç»ƒ [æ¨¡å‹ C-Ngram (LGBM + E5 + Ngram)] ===
print("\n--- æ­£åœ¨è®­ç»ƒ [æ¨¡å‹ C-Ngram] ---")

# 1. åŠ è½½æ‰€æœ‰éœ€è¦çš„ç‰¹å¾
# (æ¥è‡ª æ­¥éª¤ 7)
train_emb_e5 = np.load(os.path.join(OUTPUT_DIR, 'train_emb_e5.npy'))
# (all_4_features_train å’Œ train_ngram_diff å·²åœ¨ æ­¥éª¤ 22 åŠ è½½è¿‡)

print(f"  E5 åµŒå…¥: {train_emb_e5.shape}")
print(f"  4 ä¸ªåç½®ç‰¹å¾: {all_4_features_train.shape}")
print(f"  N-gram å·®å¼‚: {train_ngram_diff.shape}")

# 2. å †å æ‰€æœ‰ç‰¹å¾
X_C_ngram_full = hstack([
    train_emb_e5,
    all_4_features_train,
    train_ngram_diff
]).tocsr() 

print(f"âœ… æ–°çš„ C ç‰¹å¾çŸ©é˜µå·²åˆ›å»º (å½¢çŠ¶: {X_C_ngram_full.shape})")

# 3. ä½¿ç”¨ç›¸åŒçš„åˆ†å±‚æŠ½æ ·ç´¢å¼• (æ¥è‡ª æ­¥éª¤ 7)
X_train_C_ngram = X_C_ngram_full[train_indices]
X_val_C_ngram = X_C_ngram_full[val_indices]

# 4. è®­ç»ƒæ–°çš„ LGBM æ¨¡å‹
lgbm_model_C_ngram = LGBMClassifier(
    n_estimators=300, 
    learning_rate=0.05, 
    num_leaves=64, 
    random_state=42,
    device='gpu'
)

print("â³ æ­£åœ¨è®­ç»ƒ LGBM (E5 + 4 ç‰¹å¾ + N-gram)...")
lgbm_model_C_ngram.fit(X_train_C_ngram, y_train)

# 5. è¯„ä¼° (åˆ›å»ºæ–°å˜é‡)
val_preds_C_ngram = lgbm_model_C_ngram.predict_proba(X_val_C_ngram)
logloss_C_ngram = log_loss(y_val, val_preds_C_ngram)
print(f"ğŸ¯ [æ¨¡å‹ C-Ngram] Validation LogLoss: {logloss_C_ngram:.5f}")
print(f"  (åŸå§‹æ¨¡å‹ C LogLoss: {logloss_C:.5f})") # logloss_C æ¥è‡ª æ­¥éª¤ 9

# 6. ä¿å­˜æ–°æ¨¡å‹
lgbm_model_C_ngram.booster_.save_model(os.path.join(OUTPUT_DIR, 'model_C_lgbm_ngram.txt'))
print("âœ… [æ¨¡å‹ C-Ngram] å·²ä¿å­˜ä¸º 'model_C_lgbm_ngram.txt'")

del X_C_ngram_full, X_train_C_ngram, X_val_C_ngram, lgbm_model_C_ngram, train_ngram_diff
clear_memory()

# === æ­¥éª¤ 24: (æ–°) æœ€ç»ˆ 5 æ¨¡å‹é›†æˆ (ä½¿ç”¨ A/C N-gram ç‰ˆ) ===
print("\n--- æ­£åœ¨æ‰§è¡Œ: æœ€ç»ˆ 5 æ¨¡å‹é›†æˆ (ä½¿ç”¨ A-Ngram, C-Ngram) ---")

# (val_preds_B, val_preds_D, val_preds_E æ¥è‡ª æ­¥éª¤ 18)
# (val_preds_A_ngram æ¥è‡ª æ­¥éª¤ 22, val_preds_C_ngram æ¥è‡ª æ­¥éª¤ 23)

print(f"  A-Ngram Loss: {logloss_A_ngram:.5f}")
print(f"  C-Ngram Loss: {logloss_C_ngram:.5f}")

# --- ä¼˜åŒ– 5 æ¨¡å‹é›†æˆæƒé‡ (A-Ngram + B + C-Ngram + D + E) ---

def loss_fn_ensemble_5_ngram(weights):
    wA_ng, wB, wC_ng, wD = weights
    wE = 1.0 - wA_ng - wB - wC_ng - wD
    if wE < 0 or min(weights) < 0: return 100.0
    ensemble_val_preds = (
        (val_preds_A_ngram * wA_ng) +  # æ–° A
        (val_preds_B * wB) +           
        (val_preds_C_ngram * wC_ng) +  # æ–° C
        (val_preds_D * wD) +           
        (val_preds_E * wE)             
    )
    ensemble_val_preds = np.clip(ensemble_val_preds, 1e-7, 1 - 1e-7)
    return log_loss(y_val, ensemble_val_preds)

initial_weights_5 = [0.3, 0.1, 0.3, 0.1]  # [wA, wB, wC, wD]
bounds_5 = [(0, 1), (0, 1), (0, 1), (0, 1)]
constraints_5 = {'type': 'ineq', 'fun': lambda w: 1.0 - sum(w)}

res_5_ngram = minimize(
    loss_fn_ensemble_5_ngram, initial_weights_5, method='SLSQP',
    bounds=bounds_5, constraints=constraints_5
)

wA_opt_5_ng, wB_opt_5_ng, wC_opt_5_ng, wD_opt_5_ng = res_5_ngram.x
wE_opt_5_ng = 1.0 - sum(res_5_ngram.x)
print(f"\nğŸ¯ [N-gram 5 æ¨¡å‹é›†æˆ] æœ€ä½³é›†æˆéªŒè¯ LogLoss: {res_5_ngram.fun:.5f}")
print(f"  (ä¸Šä¸€æ¬¡ 5 æ¨¡å‹ LogLoss: {res_5.fun:.5f})") # res_5 æ¥è‡ª æ­¥éª¤ 18

print("--- [N-gram 5 æ¨¡å‹] æœ€ä½³æƒé‡ ---")
print(f"æ¨¡å‹ A-Ngram: {wA_opt_5_ng:.4f}")
print(f"æ¨¡å‹ B:       {wB_opt_5_ng:.4f}")
print(f"æ¨¡å‹ C-Ngram: {wC_opt_5_ng:.4f}")
print(f"æ¨¡å‹ D-Orig:  {wD_opt_5_ng:.4f}")
print(f"æ¨¡å‹ E-New:   {wE_opt_5_ng:.4f}")

# --- ä¿å­˜æœ€ç»ˆçš„ N-gram 5 æ¨¡å‹æƒé‡ ---
final_weights_5_ngram = np.array([wA_opt_5_ng, wB_opt_5_ng, wC_opt_5_ng, wD_opt_5_ng, wE_opt_5_ng])
np.save(os.path.join(OUTPUT_DIR, 'ensemble_weights_5model_ngram.npy'), final_weights_5_ngram)
print(f"âœ… æœ€ç»ˆ(N-gram 5æ¨¡å‹)æƒé‡å·²ä¿å­˜åˆ° 'ensemble_weights_5model_ngram.npy'")

