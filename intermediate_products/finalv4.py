# === å•å…ƒæ ¼ 1: å¯¼å…¥ã€è®¾ç½®ä¸è·¯å¾„å®šä¹‰ ===

import pandas as pd
import numpy as np
import os
import torch
import gc
import joblib 
import lightgbm as lgb
from scipy.sparse import hstack
import warnings

# ç‰¹å¾å·¥ç¨‹
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# è¯„ä¼°ä¸éªŒè¯
from sklearn.model_selection import StratifiedKFold # <-- å¯¼å…¥ K-Fold
from sklearn.metrics import log_loss

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings('ignore')
os.environ["WANDB_MODE"] = "disabled"

# å†…å­˜æ¸…ç†å‡½æ•°
def clear_memory():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

print("ğŸš€ (çº¯å‡€ç‰ˆ 5-Fold CV, ä»…æ¨¡å‹A) ç®¡çº¿å¯åŠ¨ï¼")

# --- 1. ç«èµ›æ•°æ®è·¯å¾„ ---
COMP_DIR = "/kaggle/input/llm-classification-finetuning"
TRAIN_FILE = os.path.join(COMP_DIR, "train.csv")
TEST_FILE = os.path.join(COMP_DIR, "test.csv")
SAMPLE_FILE = os.path.join(COMP_DIR, "sample_submission.csv")

# --- 2. å…¬å…±æ¨¡å‹è·¯å¾„ ---
# (æ ¹æ®ä½ çš„æˆªå›¾ image_8be7c0.png å’Œ finalv3 (1).ipynb)
print("â³ æ­£åœ¨å®šä¹‰å…¬å…±æ¨¡å‹è·¯å¾„...")
BASE_MINILM_PATH = "/kaggle/input/sentencetransformersallminilml6v2"
print(f"  ...MiniLM è·¯å¾„: {BASE_MINILM_PATH}")

print("âœ… æ‰€æœ‰è·¯å¾„å®šä¹‰å®Œæ¯•ã€‚")

# === å•å…ƒæ ¼ 2: ä¸€ç«™å¼ç‰¹å¾å·¥ç¨‹ (å·²ä¿®æ­£) ===

print(f"â³ æ­£åœ¨åŠ è½½ train.csv å’Œ test.csv...")
try:
    train_df = pd.read_csv(TRAIN_FILE)
    test_df = pd.read_csv(TEST_FILE)
    sample_df = pd.read_csv(SAMPLE_FILE) # sample_df åœ¨æœ€åå•å…ƒæ ¼ä¼šç”¨åˆ°
    print(f"  è®­ç»ƒé›†: {train_df.shape}, æµ‹è¯•é›†: {test_df.shape}")
except FileNotFoundError as e:
    print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥! {e}")
    raise

# --- 2.1 åŸºç¡€ç‰¹å¾ (len, lexical) ---
print("â³ æ­£åœ¨åˆ›å»ºåŸºç¡€ç‰¹å¾ (len_diff, lexical_diff)...")
def create_base_features(df):
    df['text_a'] = df['prompt'] + " " + df['response_a']
    df['text_b'] = df['prompt'] + " " + df['response_b']
    df['combined_for_embedding'] = df['text_a'] + " [SEP] " + df['text_b']
    df["resp_a_len"] = df["response_a"].str.len()
    df["resp_b_len"] = df["response_b"].str.len()
    df["len_diff"] = df["resp_a_len"] - df["resp_b_len"]
    df["len_ratio"] = df["resp_a_len"] / (df["resp_b_len"] + 1e-6)
    df["lexical_a"] = df["response_a"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))
    df["lexical_b"] = df["response_b"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))
    df["lexical_diff"] = df["lexical_a"] - df["lexical_b"]
    return df

train_df = create_base_features(train_df)
test_df = create_base_features(test_df)
train_df['label'] = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)
y_true_full = train_df['label'] # å‡†å¤‡å¥½æ‰€æœ‰æ ‡ç­¾ (57k)

# --- 2.2 SBERT åµŒå…¥ (MiniLM) ---
print("â³ æ­£åœ¨ç”Ÿæˆ MiniLM åµŒå…¥ (ç”¨äº A å’Œ ç›¸ä¼¼åº¦)...")
model_minilm = SentenceTransformer(BASE_MINILM_PATH, device='cuda') 
train_emb_minilm = model_minilm.encode(train_df['combined_for_embedding'].tolist(), show_progress_bar=True, batch_size=128)
test_emb_minilm = model_minilm.encode(test_df['combined_for_embedding'].tolist(), show_progress_bar=True, batch_size=128)

# --- 2.3 ç›¸ä¼¼åº¦ç‰¹å¾ (æ¥è‡ª MiniLM) ---
print("â³ æ­£åœ¨åˆ›å»º ç›¸ä¼¼åº¦ ç‰¹å¾...")
resp_a_emb_train = model_minilm.encode(train_df['response_a'].tolist(), show_progress_bar=True, batch_size=128)
resp_b_emb_train = model_minilm.encode(train_df['response_b'].tolist(), show_progress_bar=True, batch_size=128)
train_df['cosine_similarity'] = np.array([cosine_similarity(resp_a_emb_train[i].reshape(1, -1), resp_b_emb_train[i].reshape(1, -1))[0][0] for i in range(len(resp_a_emb_train))])

resp_a_emb_test = model_minilm.encode(test_df['response_a'].tolist(), show_progress_bar=True, batch_size=128)
resp_b_emb_test = model_minilm.encode(test_df['response_b'].tolist(), show_progress_bar=True, batch_size=128)
test_df['cosine_similarity'] = np.array([cosine_similarity(resp_a_emb_test[i].reshape(1, -1), resp_b_emb_test[i].reshape(1, -1))[0][0] for i in range(len(resp_a_emb_test))])

del model_minilm, resp_a_emb_train, resp_b_emb_train, resp_a_emb_test, resp_b_emb_test
clear_memory()

# --- 2.4 N-gram ç‰¹å¾ (å³æ—¶è®­ç»ƒ) ---
print("â³ æ­£åœ¨ (å³æ—¶) è®­ç»ƒ N-gram Vectorizer å¹¶åˆ›å»ºç‰¹å¾...")
corpus = pd.concat([train_df['response_a'], train_df['response_b']]).astype(str).unique()

vectorizer = CountVectorizer(
    max_features=2000,
    ngram_range=(1, 2), # åŒ…å« 1-grams å’Œ 2-grams
    stop_words='english',
    dtype=np.float32 
)
print("  ...æ­£åœ¨ fit Vectorizer...")
vectorizer.fit(corpus)
del corpus
clear_memory()

print("  ...æ­£åœ¨ transform è®­ç»ƒé›†/æµ‹è¯•é›†...")
# ã€ã€ã€ é€»è¾‘ä¿®å¤ ã€‘ã€‘ã€‘: æ­£ç¡®åˆ›å»ºå·®å¼‚ç‰¹å¾
train_ngram_a = vectorizer.transform(train_df['response_a'].astype(str))
train_ngram_b = vectorizer.transform(train_df['response_b'].astype(str))
train_ngram_diff = (train_ngram_a - train_ngram_b)

test_ngram_a = vectorizer.transform(test_df['response_a'].astype(str))
test_ngram_b = vectorizer.transform(test_df['response_b'].astype(str))
test_ngram_diff = (test_ngram_a - test_ngram_b)

del vectorizer, train_ngram_a, train_ngram_b, test_ngram_a, test_ngram_b
clear_memory()

# --- 2.5 ã€ã€ã€ NameError ä¿®å¤ ã€‘ã€‘ã€‘ ---
# åœ¨åˆ é™¤ train_df/test_df ä¹‹å‰ï¼Œæå– 4 ä¸ªåç½®ç‰¹å¾
print("â³ æ­£åœ¨æå– 4 ä¸ªåç½®ç‰¹å¾...")
all_4_features_train = train_df[["len_diff", "len_ratio", "lexical_diff", "cosine_similarity"]].fillna(0).values
all_4_features_test = test_df[["len_diff", "len_ratio", "lexical_diff", "cosine_similarity"]].fillna(0).values

# --- 2.6 ç°åœ¨å¯ä»¥å®‰å…¨åˆ é™¤ ---
del train_df, test_df
clear_memory()

print("âœ… æ‰€æœ‰ç‰¹å¾å·¥ç¨‹å®Œæ¯•ã€‚")

# === å•å…ƒæ ¼ 3: å‡†å¤‡æœ€ç»ˆç‰¹å¾çŸ©é˜µ ===
print("\n--- æ­£åœ¨å‡†å¤‡æœ€ç»ˆçš„è®­ç»ƒ/æµ‹è¯•ç‰¹å¾çŸ©é˜µ ---")
# (æ‰€æœ‰å˜é‡ train_emb_minilm, all_4_features_train, train_ngram_diff ç­‰éƒ½å·²åœ¨å†…å­˜ä¸­)

# --- å †å  A å¥—é¤ (MiniLM + 4 Feat + Ngram) ---
print(f"  ...å †å  A å¥—é¤ (MiniLM + 4 Feat + Ngram, å…± {384 + 4 + 2000} ç‰¹å¾)")
X_A_full = hstack([train_emb_minilm, all_4_features_train, train_ngram_diff]).tocsr()
X_test_A_ngram = hstack([test_emb_minilm, all_4_features_test, test_ngram_diff]).tocsr()

print(f"âœ… ç‰¹å¾çŸ©é˜µå‡†å¤‡å®Œæ¯•ã€‚ è®­ç»ƒé›†: {X_A_full.shape}, æµ‹è¯•é›†: {X_test_A_ngram.shape}")

# --- é‡Šæ”¾å†…å­˜ ---
del train_emb_minilm, test_emb_minilm, all_4_features_train, all_4_features_test, train_ngram_diff, test_ngram_diff
clear_memory()

# === å•å…ƒæ ¼ 4: å®šä¹‰ 5-Fold äº¤å‰éªŒè¯ ===
print("\n--- æ­£åœ¨å®šä¹‰ 5-Fold CV ---")

N_SPLITS = 5
skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)

# (y_true_full æ¥è‡ª å•å…ƒæ ¼ 2)

# å®šä¹‰ LGBM å‚æ•°
lgbm_params = {
    'n_estimators': 300,
    'learning_rate': 0.05,
    'num_leaves': 64,
    'random_state': 42,
    'device': 'gpu',
    'n_jobs': -1,
    'verbose': -1
}

# --- åˆå§‹åŒ–å­˜å‚¨å™¨ ---
oof_preds_A = np.zeros((len(y_true_full), 3))
test_preds_A_list = []

print(f"âœ… K-Fold (n_splits={N_SPLITS}) å·²å‡†å¤‡å°±ç»ªã€‚")

# === å•å…ƒæ ¼ 5: æ‰§è¡Œ 5-Fold CV è®­ç»ƒå¾ªç¯ (ä»…æ¨¡å‹A) ===
print("\n--- å¯åŠ¨ 5-Fold äº¤å‰éªŒè¯è®­ç»ƒ (ä»…æ¨¡å‹A) ---")

for fold, (train_indices, val_indices) in enumerate(skf.split(X_A_full, y_true_full)):
    print(f"\n--- æ­£åœ¨è®­ç»ƒ Fold {fold+1}/{N_SPLITS} ---")
    
    # --- å‡†å¤‡è¯¥ Fold çš„æ•°æ® ---
    y_train_fold = y_true_full.iloc[train_indices]
    y_val_fold = y_true_full.iloc[val_indices]
    
    X_train_A_fold = X_A_full[train_indices]
    X_val_A_fold = X_A_full[val_indices]

    # --- è®­ç»ƒæ¨¡å‹ A (Fold {fold+1}) ---
    print(f"  â³ è®­ç»ƒ LGBM-A (Fold {fold+1})...")
    model_A_fold = lgb.LGBMClassifier(**lgbm_params)
    model_A_fold.fit(X_train_A_fold, y_train_fold,
                     eval_set=[(X_val_A_fold, y_val_fold)],
                     eval_metric='logloss',
                     callbacks=[lgb.early_stopping(15, verbose=False)])
    
    # é¢„æµ‹éªŒè¯é›† (ç”¨äº OOF)
    oof_preds_A[val_indices] = model_A_fold.predict_proba(X_val_A_fold)
    # é¢„æµ‹æµ‹è¯•é›†
    test_preds_A_list.append(model_A_fold.predict_proba(X_test_A_ngram))
    
    print(f"  âœ… Fold {fold+1} å®Œæˆã€‚")
    del model_A_fold, X_train_A_fold, X_val_A_fold
    clear_memory()

print("\nğŸ‰ 5-Fold CV è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")

# === å•å…ƒæ ¼ 6: OOF éªŒè¯ ä¸ æäº¤ ===

print("\n--- æ­£åœ¨è®¡ç®—å®Œæ•´çš„ OOF éªŒè¯åˆ†æ•° ---")
# (oof_preds_A, y_true_full å‡å·²å‡†å¤‡å¥½)

assert not np.any(np.sum(oof_preds_A, axis=1) == 0), "OOF A ä¸­æœ‰æœªé¢„æµ‹çš„è¡Œï¼"
oof_logloss_A = log_loss(y_true_full, oof_preds_A)
print(f"ğŸ¯ æœ€ä½³ [å®Œæ•´ OOF] æ¨¡å‹ A-Ngram LogLoss: {oof_logloss_A:.5f}")

# --- èšåˆæµ‹è¯•é›†é¢„æµ‹ ---
print("\n--- æ­£åœ¨èšåˆ 5-Fold çš„æµ‹è¯•é›†é¢„æµ‹ ---")
final_preds = np.mean(test_preds_A_list, axis=0)
print(f"  ...æµ‹è¯•é›†é¢„æµ‹å·²å¹³å‡ã€‚ å½¢çŠ¶: {final_preds.shape}")

# --- ç”Ÿæˆæœ€ç»ˆçš„ submission.csv ---
print("\n--- æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæäº¤æ–‡ä»¶ ---")
final_preds = final_preds / final_preds.sum(axis=1, keepdims=True)
final_preds = np.clip(final_preds, 1e-7, 1 - 1e-7)

submission_final = pd.DataFrame(final_preds, columns=sample_df.columns[1:])
submission_final.insert(0, "id", sample_df["id"])

assert len(submission_final) == len(sample_df), "æäº¤æ–‡ä»¶è¡Œæ•°ä¸åŒ¹é…!"
submission_final.to_csv("submission.csv", index=False) 

print("\nğŸ‰ğŸ‰ğŸ‰ æœ€ç»ˆçš„ (5-Fold CV, ä»…æ¨¡å‹A-Ngram) submission.csv å·²ç”Ÿæˆï¼ğŸ‰ğŸ‰ğŸ‰")
print(submission_final.head())

