{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13571239,"sourceType":"datasetVersion","datasetId":8621307}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === å•å…ƒæ ¼ 1: å¯¼å…¥ã€è®¾ç½®ä¸è·¯å¾„å®šä¹‰ ===\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\nimport shutil\nimport gc\nimport joblib \nimport lightgbm as lgb\nfrom scipy.sparse import hstack, load_npz\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import (\n    AutoModelForSequenceClassification, AutoTokenizer, Trainer, \n    TrainingArguments\n)\nfrom peft import PeftModel\nfrom datasets import Dataset\n\n# ç¦ç”¨ wandb\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\n# å†…å­˜æ¸…ç†å‡½æ•°\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nprint(\"ğŸš€ Kaggle æ¨ç† Notebook å¯åŠ¨ï¼\")\n\n# --- 1. ç«èµ›æ•°æ®è·¯å¾„ ---\nCOMP_DIR = \"/kaggle/input/llm-classification-finetuning\"\nTEST_FILE = os.path.join(COMP_DIR, \"test.csv\")\nSAMPLE_FILE = os.path.join(COMP_DIR, \"sample_submission.csv\")\n\n# --- 2. ä½ çš„æ•°æ®é›†è·¯å¾„ ---\n# (è¿™ä¸ä½ çš„æˆªå›¾ llm-proj2-team8 ä¸€è‡´)\nYOUR_DATASET_NAME = \"llm-proj2-team8\" \nDATASET_DIR = os.path.join(\"/kaggle/input\", YOUR_DATASET_NAME)\n\n# --- 3. ã€ã€ã€ è·¯å¾„ä¿®æ­£ ã€‘ã€‘ã€‘ ---\n# (æ ¹æ®ä½ çš„æˆªå›¾ï¼Œæ‰€æœ‰è·¯å¾„éƒ½å·²ä¿®æ­£)\n\n# A. ä½ è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (åœ¨ 'output/' å­æ–‡ä»¶å¤¹ä¸­)\nOUTPUT_DIR = os.path.join(DATASET_DIR, \"output\")\nMODEL_A_PATH = os.path.join(OUTPUT_DIR, \"model_A_lgbm_ngram.txt\")\nMODEL_C_PATH = os.path.join(OUTPUT_DIR, \"model_C_lgbm_ngram.txt\")\nADAPTER_B_PATH = os.path.join(OUTPUT_DIR, \"model_B_deberta_lora\")\nADAPTER_D_PATH = os.path.join(OUTPUT_DIR, \"model_D_roberta_lora\")\nADAPTER_E_PATH = os.path.join(OUTPUT_DIR, \"model_E_deberta_base_lora\")\n\nTEMP_B_PATH = os.path.join(OUTPUT_DIR, \"temp_B.npy\")\nTEMP_D_PATH = os.path.join(OUTPUT_DIR, \"temp_D.npy\")\nTEMP_E_PATH = os.path.join(OUTPUT_DIR, \"temp_E.npy\")\nVECTORIZER_PATH = os.path.join(OUTPUT_DIR, \"vectorizer_ngram.joblib\")\n\n# ã€è¯·ç¡®è®¤!ã€‘æˆ‘å‡è®¾ä½ ä¸Šä¼ çš„æ˜¯ ngram ç‰ˆæœ¬ï¼Œå¦‚æœä¸æ˜¯ï¼Œè¯·æ”¹ä¸º 'ensemble_weights_5model.npy'\nWEIGHTS_PATH = os.path.join(OUTPUT_DIR, \"ensemble_weights_5model_ngram.npy\") \n\n# B. åŸºç¡€æ¨¡å‹è·¯å¾„ (åœ¨ 'model/' å­æ–‡ä»¶å¤¹ä¸­)\nBASE_MODEL_DIR = os.path.join(DATASET_DIR, \"model\")\nBASE_MINILM_PATH = os.path.join(BASE_MODEL_DIR, \"sentencetransformersallminilml6v2\")\nBASE_E5_PATH = os.path.join(BASE_MODEL_DIR, \"e5-small-v2\")\nBASE_B_PATH = os.path.join(BASE_MODEL_DIR, \"deberta-v3-small\")\nBASE_D_PATH = os.path.join(BASE_MODEL_DIR, \"roberta-transformers-pytorch\")\nBASE_E_PATH = os.path.join(BASE_MODEL_DIR, \"deberta-v3-base\")\n\nprint(\"âœ… æ‰€æœ‰è·¯å¾„å®šä¹‰å®Œæ¯•ã€‚\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:37.680289Z","iopub.execute_input":"2025-10-31T19:26:37.680886Z","iopub.status.idle":"2025-10-31T19:26:37.690882Z","shell.execute_reply.started":"2025-10-31T19:26:37.680862Z","shell.execute_reply":"2025-10-31T19:26:37.690159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === å•å…ƒæ ¼ 2: åŠ è½½æ•°æ®ä¸ç‰¹å¾å·¥ç¨‹ (LGBM A/C) ===\n\nprint(f\"â³ æ­£åœ¨åŠ è½½ test.csv...\")\ntest_df = pd.read_csv(TEST_FILE)\nsample_df = pd.read_csv(SAMPLE_FILE)\n\nprint(\"â³ æ­£åœ¨ä¸º LGBM A/C è¿›è¡Œç‰¹å¾å·¥ç¨‹...\")\n\n# 3.1 åŸºç¡€åç½®ç‰¹å¾\ndef create_base_features(df):\n    df['text_a'] = df['prompt'] + \" \" + df['response_a']\n    df['text_b'] = df['prompt'] + \" \" + df['response_b']\n    df['combined_for_embedding'] = df['text_a'] + \" [SEP] \" + df['text_b']\n    df[\"resp_a_len\"] = df[\"response_a\"].str.len()\n    df[\"resp_b_len\"] = df[\"response_b\"].str.len()\n    df[\"len_diff\"] = df[\"resp_a_len\"] - df[\"resp_b_len\"]\n    df[\"len_ratio\"] = df[\"resp_a_len\"] / (df[\"resp_b_len\"] + 1e-6)\n    df[\"lexical_a\"] = df[\"response_a\"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))\n    df[\"lexical_b\"] = df[\"response_b\"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))\n    df[\"lexical_diff\"] = df[\"lexical_a\"] - df[\"lexical_b\"]\n    return df\n\ntest_df = create_base_features(test_df)\nprint(\"  ...åŸºç¡€åç½®ç‰¹å¾å®Œæˆã€‚\")\n\n# 3.2 åµŒå…¥å’Œä½™å¼¦ç›¸ä¼¼åº¦\nprint(\"  ...æ­£åœ¨ç”Ÿæˆ MiniLM åµŒå…¥ (ç”¨äº A å’Œ 4å·ç‰¹å¾)\")\nmodel_minilm = SentenceTransformer(BASE_MINILM_PATH, device='cuda')\ntest_emb_minilm = model_minilm.encode(\n    test_df['combined_for_embedding'].tolist(), \n    show_progress_bar=True, batch_size=128, convert_to_numpy=True\n)\nresp_a_emb_test = model_minilm.encode(test_df['response_a'].tolist(), batch_size=128)\nresp_b_emb_test = model_minilm.encode(test_df['response_b'].tolist(), batch_size=128)\ntest_df['cosine_similarity'] = np.array([\n    cosine_similarity(resp_a_emb_test[i].reshape(1, -1), resp_b_emb_test[i].reshape(1, -1))[0][0] \n    for i in range(len(resp_a_emb_test))\n])\ndel model_minilm, resp_a_emb_test, resp_b_emb_test; clear_memory()\nprint(\"  ...MiniLM åµŒå…¥å®Œæˆã€‚\")\n\nprint(\"  ...æ­£åœ¨ç”Ÿæˆ E5 åµŒå…¥ (ç”¨äº C)\")\nmodel_e5 = SentenceTransformer(BASE_E5_PATH, device='cuda')\ntest_emb_e5 = model_e5.encode(\n    test_df[\"combined_for_embedding\"].tolist(), \n    batch_size=128, show_progress_bar=True, convert_to_numpy=True\n)\ndel model_e5; clear_memory()\nprint(\"  ...E5 åµŒå…¥å®Œæˆã€‚\")\n\n# 3.3 N-gram ç‰¹å¾\nprint(\"  ...æ­£åœ¨ç”Ÿæˆ N-gram å·®å¼‚ç‰¹å¾\")\nvectorizer = joblib.load(VECTORIZER_PATH)\ntest_ngram_a = vectorizer.transform(test_df['response_a'].astype(str))\ntest_ngram_b = vectorizer.transform(test_df['response_b'].astype(str))\ntest_ngram_diff = (test_ngram_a - test_ngram_b)\nprint(\"  ...N-gram ç‰¹å¾å®Œæˆã€‚\")\n\n# 3.4 å †å ç‰¹å¾\nall_4_features_test = test_df[[\"len_diff\", \"len_ratio\", \"lexical_diff\", \"cosine_similarity\"]].fillna(0).values\n\nX_test_A = hstack([test_emb_minilm, all_4_features_test, test_ngram_diff]).tocsr()\nX_test_C = hstack([test_emb_e5, all_4_features_test, test_ngram_diff]).tocsr()\nprint(\"  ...ç‰¹å¾å †å å®Œæˆã€‚\")\n\ndel test_emb_minilm, test_emb_e5, all_4_features_test, test_ngram_diff; clear_memory()\nprint(\"âœ… LGBM A/C ç‰¹å¾å·¥ç¨‹å®Œæ¯•ã€‚\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:37.692178Z","iopub.execute_input":"2025-10-31T19:26:37.692455Z","iopub.status.idle":"2025-10-31T19:26:39.781019Z","shell.execute_reply.started":"2025-10-31T19:26:37.692421Z","shell.execute_reply":"2025-10-31T19:26:39.780286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === å•å…ƒæ ¼ 3: LGBM A/C æ¨ç† ===\n\nprint(\"â³ æ­£åœ¨ä½¿ç”¨ LGBM A/C è¿›è¡Œæ¨ç†...\")\nlgbm_model_A = lgb.Booster(model_file=MODEL_A_PATH)\npreds_A = lgbm_model_A.predict(X_test_A)\n\nlgbm_model_C = lgb.Booster(model_file=MODEL_C_PATH)\npreds_C = lgbm_model_C.predict(X_test_C)\n\ndel X_test_A, X_test_C, lgbm_model_A, lgbm_model_C; clear_memory()\nprint(\"âœ… LGBM A/C æ¨ç†å®Œæ¯•ã€‚\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:39.781784Z","iopub.execute_input":"2025-10-31T19:26:39.782040Z","iopub.status.idle":"2025-10-31T19:26:40.300146Z","shell.execute_reply.started":"2025-10-31T19:26:39.782020Z","shell.execute_reply":"2025-10-31T19:26:40.299503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === å•å…ƒæ ¼ 4: Transformer B/D/E æ¨ç† ===\n\nprint(\"â³ æ­£åœ¨ä¸º Transformers B/D/E å‡†å¤‡æ•°æ®...\")\ntest_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'response_a', 'response_b']])\n\ntraining_args = TrainingArguments(\n    output_dir=\"./infer_results\",\n    per_device_eval_batch_size=16,\n    dataloader_num_workers=0,\n    fp16=True,\n    fp16_full_eval=True,\n    report_to=[]\n)\n\n# --- æ¨¡å‹ B (DeBERTa-small) ---\nprint(\"  ...æ­£åœ¨ä½¿ç”¨ LoRA B (DeBERTa-small) è¿›è¡Œæ¨ç†\")\ntokenizer_B = AutoTokenizer.from_pretrained(BASE_B_PATH, local_files_only=True)\ndef preprocess_function_B(examples):\n    texts = [f\"é—®é¢˜: {p} [SEP] A: {a} [SEP] B: {b}\" for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])]\n    return tokenizer_B(texts, truncation=True, padding=\"max_length\", max_length=256)\ntokenized_test_B = test_dataset.map(preprocess_function_B, batched=True, desc=\"Tokenizing B\")\n\nbase_model_B = AutoModelForSequenceClassification.from_pretrained(BASE_B_PATH, num_labels=3, local_files_only=True)\nmodel_B = PeftModel.from_pretrained(base_model_B, ADAPTER_B_PATH)\ntrainer_B = Trainer(model=model_B, args=training_args, tokenizer=tokenizer_B)\nlogits_B = trainer_B.predict(tokenized_test_B).predictions\ndel base_model_B, model_B, trainer_B, tokenizer_B; clear_memory()\nprint(\"  ...æ¨¡å‹ B æ¨ç†å®Œæ¯•ã€‚\")\n\n# --- æ¨¡å‹ D (RoBERTa-base) ---\nprint(\"  ...æ­£åœ¨ä½¿ç”¨ LoRA D (RoBERTa-base) è¿›è¡Œæ¨ç†\")\ntokenizer_D = AutoTokenizer.from_pretrained(BASE_D_PATH, local_files_only=True)\ndef preprocess_function_D(examples):\n    texts = [f\"é—®é¢˜: {p} [SEP] A: {a} [SEP] B: {b}\" for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])]\n    return tokenizer_D(texts, truncation=True, padding=\"max_length\", max_length=256)\ntokenized_test_D = test_dataset.map(preprocess_function_D, batched=True, desc=\"Tokenizing D\")\n\nbase_model_D = AutoModelForSequenceClassification.from_pretrained(BASE_D_PATH, num_labels=3, local_files_only=True)\nmodel_D = PeftModel.from_pretrained(base_model_D, ADAPTER_D_PATH)\ntrainer_D = Trainer(model=model_D, args=training_args, tokenizer=tokenizer_D)\nlogits_D = trainer_D.predict(tokenized_test_D).predictions\ndel base_model_D, model_D, trainer_D, tokenizer_D; clear_memory()\nprint(\"  ...æ¨¡å‹ D æ¨ç†å®Œæ¯•ã€‚\")\n\n# --- æ¨¡å‹ E (DeBERTa-base) ---\nprint(\"  ...æ­£åœ¨ä½¿ç”¨ LoRA E (DeBERTa-base) è¿›è¡Œæ¨ç†\")\ntokenizer_E = AutoTokenizer.from_pretrained(BASE_E_PATH, local_files_only=True)\ndef preprocess_function_E(examples):\n    texts = [f\"é—®é¢˜: {p} [SEP] A: {a} [SEP] B: {b}\" for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])]\n    return tokenizer_E(texts, truncation=True, padding=\"max_length\", max_length=256)\ntokenized_test_E = test_dataset.map(preprocess_function_E, batched=True, desc=\"Tokenizing E\")\n\nbase_model_E = AutoModelForSequenceClassification.from_pretrained(BASE_E_PATH, num_labels=3, local_files_only=True)\nmodel_E = PeftModel.from_pretrained(base_model_E, ADAPTER_E_PATH)\ntrainer_E = Trainer(model=model_E, args=training_args, tokenizer=tokenizer_E)\nlogits_E = trainer_E.predict(tokenized_test_E).predictions\ndel base_model_E, model_E, trainer_E, tokenizer_E; clear_memory()\nprint(\"  ...æ¨¡å‹ E æ¨ç†å®Œæ¯•ã€‚\")\n\nprint(\"âœ… Transformer B/D/E æ¨ç†å®Œæ¯•ã€‚\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:40.300910Z","iopub.execute_input":"2025-10-31T19:26:40.301172Z","iopub.status.idle":"2025-10-31T19:26:48.991214Z","shell.execute_reply.started":"2025-10-31T19:26:40.301155Z","shell.execute_reply":"2025-10-31T19:26:48.990422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === å•å…ƒæ ¼ 5: æ ¡å‡†ã€é›†æˆå¹¶ç”Ÿæˆ submission.csv ===\n\nprint(\"â³ æ­£åœ¨æ ¡å‡†å’Œé›†æˆæ‰€æœ‰ 5 ä¸ªæ¨¡å‹...\")\n\ndef temperature_scale(logits, T):\n    logits_T = logits / T\n    return torch.softmax(torch.tensor(logits_T), dim=-1).numpy()\n\n# åŠ è½½æ¸©åº¦\nT_B = np.load(TEMP_B_PATH)[0]\nT_D = np.load(TEMP_D_PATH)[0]\nT_E = np.load(TEMP_E_PATH)[0]\nprint(f\"  ...å·²åŠ è½½æ¸©åº¦: T_B={T_B:.3f}, T_D={T_D:.3f}, T_E={T_E:.3f}\")\n\n# æ ¡å‡†\npreds_B = temperature_scale(logits_B, T_B)\npreds_D = temperature_scale(logits_D, T_D)\npreds_E = temperature_scale(logits_E, T_E)\nprint(\"  ...B, D, E æ ¡å‡†å®Œæ¯•ã€‚\")\n\n# åŠ è½½æƒé‡\nW = np.load(WEIGHTS_PATH) # [wA, wB, wC, wD, wE]\nprint(f\"  ...å·²åŠ è½½ N-gram 5 æ¨¡å‹æƒé‡:\")\nprint(f\"    A-Ng: {W[0]:.4f}, B: {W[1]:.4f}, C-Ng: {W[2]:.4f}, D: {W[3]:.4f}, E: {W[4]:.4f}\")\n\n# æœ€ç»ˆé›†æˆ (preds_A å’Œ preds_C å·²ç»æ˜¯æ¦‚ç‡ï¼Œæ— éœ€æ ¡å‡†)\nfinal_preds = (\n    (preds_A * W[0]) + (preds_B * W[1]) +\n    (preds_C * W[2]) + (preds_D * W[3]) +\n    (preds_E * W[4])\n)\nprint(\"  ...é›†æˆå®Œæ¯•ã€‚\")\n\n# === 7. åˆ›å»ºæäº¤æ–‡ä»¶ ===\nprint(\"â³ æ­£åœ¨åˆ›å»º submission.csv...\")\nsubmission_df = pd.DataFrame(final_preds, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\nsubmission_df['id'] = test_df['id']\nsubmission_df = submission_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n\n# ç¡®ä¿æ¦‚ç‡æ€»å’Œä¸º 1 (SLSQP ä¼˜åŒ–å™¨ä¼šä¿è¯æ€»å’Œä¸º1ï¼Œä½†ä¿é™©èµ·è§)\nsubmission_df.iloc[:, 1:] = submission_df.iloc[:, 1:].div(submission_df.iloc[:, 1:].sum(axis=1), axis=0)\n# è£å‰ªä»¥é¿å… LogLoss é”™è¯¯\nsubmission_df.iloc[:, 1:] = np.clip(submission_df.iloc[:, 1:], 1e-7, 1 - 1e-7)\n\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"âœ… æäº¤æ–‡ä»¶åˆ›å»ºæˆåŠŸ!\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:48.992725Z","iopub.execute_input":"2025-10-31T19:26:48.992931Z","iopub.status.idle":"2025-10-31T19:26:49.016802Z","shell.execute_reply.started":"2025-10-31T19:26:48.992917Z","shell.execute_reply":"2025-10-31T19:26:49.016034Z"}},"outputs":[],"execution_count":null}]}