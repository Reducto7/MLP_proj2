{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13571239,"sourceType":"datasetVersion","datasetId":8621307}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === 单元格 1: 导入、设置与路径定义 ===\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\nimport shutil\nimport gc\nimport joblib \nimport lightgbm as lgb\nfrom scipy.sparse import hstack, load_npz\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import (\n    AutoModelForSequenceClassification, AutoTokenizer, Trainer, \n    TrainingArguments\n)\nfrom peft import PeftModel\nfrom datasets import Dataset\n\n# 禁用 wandb\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\n# 内存清理函数\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nprint(\"🚀 Kaggle 推理 Notebook 启动！\")\n\n# --- 1. 竞赛数据路径 ---\nCOMP_DIR = \"/kaggle/input/llm-classification-finetuning\"\nTEST_FILE = os.path.join(COMP_DIR, \"test.csv\")\nSAMPLE_FILE = os.path.join(COMP_DIR, \"sample_submission.csv\")\n\n# --- 2. 你的数据集路径 ---\n# (这与你的截图 llm-proj2-team8 一致)\nYOUR_DATASET_NAME = \"llm-proj2-team8\" \nDATASET_DIR = os.path.join(\"/kaggle/input\", YOUR_DATASET_NAME)\n\n# --- 3. 【【【 路径修正 】】】 ---\n# (根据你的截图，所有路径都已修正)\n\n# A. 你训练好的模型路径 (在 'output/' 子文件夹中)\nOUTPUT_DIR = os.path.join(DATASET_DIR, \"output\")\nMODEL_A_PATH = os.path.join(OUTPUT_DIR, \"model_A_lgbm_ngram.txt\")\nMODEL_C_PATH = os.path.join(OUTPUT_DIR, \"model_C_lgbm_ngram.txt\")\nADAPTER_B_PATH = os.path.join(OUTPUT_DIR, \"model_B_deberta_lora\")\nADAPTER_D_PATH = os.path.join(OUTPUT_DIR, \"model_D_roberta_lora\")\nADAPTER_E_PATH = os.path.join(OUTPUT_DIR, \"model_E_deberta_base_lora\")\n\nTEMP_B_PATH = os.path.join(OUTPUT_DIR, \"temp_B.npy\")\nTEMP_D_PATH = os.path.join(OUTPUT_DIR, \"temp_D.npy\")\nTEMP_E_PATH = os.path.join(OUTPUT_DIR, \"temp_E.npy\")\nVECTORIZER_PATH = os.path.join(OUTPUT_DIR, \"vectorizer_ngram.joblib\")\n\n# 【请确认!】我假设你上传的是 ngram 版本，如果不是，请改为 'ensemble_weights_5model.npy'\nWEIGHTS_PATH = os.path.join(OUTPUT_DIR, \"ensemble_weights_5model_ngram.npy\") \n\n# B. 基础模型路径 (在 'model/' 子文件夹中)\nBASE_MODEL_DIR = os.path.join(DATASET_DIR, \"model\")\nBASE_MINILM_PATH = os.path.join(BASE_MODEL_DIR, \"sentencetransformersallminilml6v2\")\nBASE_E5_PATH = os.path.join(BASE_MODEL_DIR, \"e5-small-v2\")\nBASE_B_PATH = os.path.join(BASE_MODEL_DIR, \"deberta-v3-small\")\nBASE_D_PATH = os.path.join(BASE_MODEL_DIR, \"roberta-transformers-pytorch\")\nBASE_E_PATH = os.path.join(BASE_MODEL_DIR, \"deberta-v3-base\")\n\nprint(\"✅ 所有路径定义完毕。\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:37.680289Z","iopub.execute_input":"2025-10-31T19:26:37.680886Z","iopub.status.idle":"2025-10-31T19:26:37.690882Z","shell.execute_reply.started":"2025-10-31T19:26:37.680862Z","shell.execute_reply":"2025-10-31T19:26:37.690159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 单元格 2: 加载数据与特征工程 (LGBM A/C) ===\n\nprint(f\"⏳ 正在加载 test.csv...\")\ntest_df = pd.read_csv(TEST_FILE)\nsample_df = pd.read_csv(SAMPLE_FILE)\n\nprint(\"⏳ 正在为 LGBM A/C 进行特征工程...\")\n\n# 3.1 基础偏置特征\ndef create_base_features(df):\n    df['text_a'] = df['prompt'] + \" \" + df['response_a']\n    df['text_b'] = df['prompt'] + \" \" + df['response_b']\n    df['combined_for_embedding'] = df['text_a'] + \" [SEP] \" + df['text_b']\n    df[\"resp_a_len\"] = df[\"response_a\"].str.len()\n    df[\"resp_b_len\"] = df[\"response_b\"].str.len()\n    df[\"len_diff\"] = df[\"resp_a_len\"] - df[\"resp_b_len\"]\n    df[\"len_ratio\"] = df[\"resp_a_len\"] / (df[\"resp_b_len\"] + 1e-6)\n    df[\"lexical_a\"] = df[\"response_a\"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))\n    df[\"lexical_b\"] = df[\"response_b\"].apply(lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1e-6))\n    df[\"lexical_diff\"] = df[\"lexical_a\"] - df[\"lexical_b\"]\n    return df\n\ntest_df = create_base_features(test_df)\nprint(\"  ...基础偏置特征完成。\")\n\n# 3.2 嵌入和余弦相似度\nprint(\"  ...正在生成 MiniLM 嵌入 (用于 A 和 4号特征)\")\nmodel_minilm = SentenceTransformer(BASE_MINILM_PATH, device='cuda')\ntest_emb_minilm = model_minilm.encode(\n    test_df['combined_for_embedding'].tolist(), \n    show_progress_bar=True, batch_size=128, convert_to_numpy=True\n)\nresp_a_emb_test = model_minilm.encode(test_df['response_a'].tolist(), batch_size=128)\nresp_b_emb_test = model_minilm.encode(test_df['response_b'].tolist(), batch_size=128)\ntest_df['cosine_similarity'] = np.array([\n    cosine_similarity(resp_a_emb_test[i].reshape(1, -1), resp_b_emb_test[i].reshape(1, -1))[0][0] \n    for i in range(len(resp_a_emb_test))\n])\ndel model_minilm, resp_a_emb_test, resp_b_emb_test; clear_memory()\nprint(\"  ...MiniLM 嵌入完成。\")\n\nprint(\"  ...正在生成 E5 嵌入 (用于 C)\")\nmodel_e5 = SentenceTransformer(BASE_E5_PATH, device='cuda')\ntest_emb_e5 = model_e5.encode(\n    test_df[\"combined_for_embedding\"].tolist(), \n    batch_size=128, show_progress_bar=True, convert_to_numpy=True\n)\ndel model_e5; clear_memory()\nprint(\"  ...E5 嵌入完成。\")\n\n# 3.3 N-gram 特征\nprint(\"  ...正在生成 N-gram 差异特征\")\nvectorizer = joblib.load(VECTORIZER_PATH)\ntest_ngram_a = vectorizer.transform(test_df['response_a'].astype(str))\ntest_ngram_b = vectorizer.transform(test_df['response_b'].astype(str))\ntest_ngram_diff = (test_ngram_a - test_ngram_b)\nprint(\"  ...N-gram 特征完成。\")\n\n# 3.4 堆叠特征\nall_4_features_test = test_df[[\"len_diff\", \"len_ratio\", \"lexical_diff\", \"cosine_similarity\"]].fillna(0).values\n\nX_test_A = hstack([test_emb_minilm, all_4_features_test, test_ngram_diff]).tocsr()\nX_test_C = hstack([test_emb_e5, all_4_features_test, test_ngram_diff]).tocsr()\nprint(\"  ...特征堆叠完成。\")\n\ndel test_emb_minilm, test_emb_e5, all_4_features_test, test_ngram_diff; clear_memory()\nprint(\"✅ LGBM A/C 特征工程完毕。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:37.692178Z","iopub.execute_input":"2025-10-31T19:26:37.692455Z","iopub.status.idle":"2025-10-31T19:26:39.781019Z","shell.execute_reply.started":"2025-10-31T19:26:37.692421Z","shell.execute_reply":"2025-10-31T19:26:39.780286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 单元格 3: LGBM A/C 推理 ===\n\nprint(\"⏳ 正在使用 LGBM A/C 进行推理...\")\nlgbm_model_A = lgb.Booster(model_file=MODEL_A_PATH)\npreds_A = lgbm_model_A.predict(X_test_A)\n\nlgbm_model_C = lgb.Booster(model_file=MODEL_C_PATH)\npreds_C = lgbm_model_C.predict(X_test_C)\n\ndel X_test_A, X_test_C, lgbm_model_A, lgbm_model_C; clear_memory()\nprint(\"✅ LGBM A/C 推理完毕。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:39.781784Z","iopub.execute_input":"2025-10-31T19:26:39.782040Z","iopub.status.idle":"2025-10-31T19:26:40.300146Z","shell.execute_reply.started":"2025-10-31T19:26:39.782020Z","shell.execute_reply":"2025-10-31T19:26:40.299503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 单元格 4: Transformer B/D/E 推理 ===\n\nprint(\"⏳ 正在为 Transformers B/D/E 准备数据...\")\ntest_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'response_a', 'response_b']])\n\ntraining_args = TrainingArguments(\n    output_dir=\"./infer_results\",\n    per_device_eval_batch_size=16,\n    dataloader_num_workers=0,\n    fp16=True,\n    fp16_full_eval=True,\n    report_to=[]\n)\n\n# --- 模型 B (DeBERTa-small) ---\nprint(\"  ...正在使用 LoRA B (DeBERTa-small) 进行推理\")\ntokenizer_B = AutoTokenizer.from_pretrained(BASE_B_PATH, local_files_only=True)\ndef preprocess_function_B(examples):\n    texts = [f\"问题: {p} [SEP] A: {a} [SEP] B: {b}\" for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])]\n    return tokenizer_B(texts, truncation=True, padding=\"max_length\", max_length=256)\ntokenized_test_B = test_dataset.map(preprocess_function_B, batched=True, desc=\"Tokenizing B\")\n\nbase_model_B = AutoModelForSequenceClassification.from_pretrained(BASE_B_PATH, num_labels=3, local_files_only=True)\nmodel_B = PeftModel.from_pretrained(base_model_B, ADAPTER_B_PATH)\ntrainer_B = Trainer(model=model_B, args=training_args, tokenizer=tokenizer_B)\nlogits_B = trainer_B.predict(tokenized_test_B).predictions\ndel base_model_B, model_B, trainer_B, tokenizer_B; clear_memory()\nprint(\"  ...模型 B 推理完毕。\")\n\n# --- 模型 D (RoBERTa-base) ---\nprint(\"  ...正在使用 LoRA D (RoBERTa-base) 进行推理\")\ntokenizer_D = AutoTokenizer.from_pretrained(BASE_D_PATH, local_files_only=True)\ndef preprocess_function_D(examples):\n    texts = [f\"问题: {p} [SEP] A: {a} [SEP] B: {b}\" for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])]\n    return tokenizer_D(texts, truncation=True, padding=\"max_length\", max_length=256)\ntokenized_test_D = test_dataset.map(preprocess_function_D, batched=True, desc=\"Tokenizing D\")\n\nbase_model_D = AutoModelForSequenceClassification.from_pretrained(BASE_D_PATH, num_labels=3, local_files_only=True)\nmodel_D = PeftModel.from_pretrained(base_model_D, ADAPTER_D_PATH)\ntrainer_D = Trainer(model=model_D, args=training_args, tokenizer=tokenizer_D)\nlogits_D = trainer_D.predict(tokenized_test_D).predictions\ndel base_model_D, model_D, trainer_D, tokenizer_D; clear_memory()\nprint(\"  ...模型 D 推理完毕。\")\n\n# --- 模型 E (DeBERTa-base) ---\nprint(\"  ...正在使用 LoRA E (DeBERTa-base) 进行推理\")\ntokenizer_E = AutoTokenizer.from_pretrained(BASE_E_PATH, local_files_only=True)\ndef preprocess_function_E(examples):\n    texts = [f\"问题: {p} [SEP] A: {a} [SEP] B: {b}\" for p, a, b in zip(examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"])]\n    return tokenizer_E(texts, truncation=True, padding=\"max_length\", max_length=256)\ntokenized_test_E = test_dataset.map(preprocess_function_E, batched=True, desc=\"Tokenizing E\")\n\nbase_model_E = AutoModelForSequenceClassification.from_pretrained(BASE_E_PATH, num_labels=3, local_files_only=True)\nmodel_E = PeftModel.from_pretrained(base_model_E, ADAPTER_E_PATH)\ntrainer_E = Trainer(model=model_E, args=training_args, tokenizer=tokenizer_E)\nlogits_E = trainer_E.predict(tokenized_test_E).predictions\ndel base_model_E, model_E, trainer_E, tokenizer_E; clear_memory()\nprint(\"  ...模型 E 推理完毕。\")\n\nprint(\"✅ Transformer B/D/E 推理完毕。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:40.300910Z","iopub.execute_input":"2025-10-31T19:26:40.301172Z","iopub.status.idle":"2025-10-31T19:26:48.991214Z","shell.execute_reply.started":"2025-10-31T19:26:40.301155Z","shell.execute_reply":"2025-10-31T19:26:48.990422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 单元格 5: 校准、集成并生成 submission.csv ===\n\nprint(\"⏳ 正在校准和集成所有 5 个模型...\")\n\ndef temperature_scale(logits, T):\n    logits_T = logits / T\n    return torch.softmax(torch.tensor(logits_T), dim=-1).numpy()\n\n# 加载温度\nT_B = np.load(TEMP_B_PATH)[0]\nT_D = np.load(TEMP_D_PATH)[0]\nT_E = np.load(TEMP_E_PATH)[0]\nprint(f\"  ...已加载温度: T_B={T_B:.3f}, T_D={T_D:.3f}, T_E={T_E:.3f}\")\n\n# 校准\npreds_B = temperature_scale(logits_B, T_B)\npreds_D = temperature_scale(logits_D, T_D)\npreds_E = temperature_scale(logits_E, T_E)\nprint(\"  ...B, D, E 校准完毕。\")\n\n# 加载权重\nW = np.load(WEIGHTS_PATH) # [wA, wB, wC, wD, wE]\nprint(f\"  ...已加载 N-gram 5 模型权重:\")\nprint(f\"    A-Ng: {W[0]:.4f}, B: {W[1]:.4f}, C-Ng: {W[2]:.4f}, D: {W[3]:.4f}, E: {W[4]:.4f}\")\n\n# 最终集成 (preds_A 和 preds_C 已经是概率，无需校准)\nfinal_preds = (\n    (preds_A * W[0]) + (preds_B * W[1]) +\n    (preds_C * W[2]) + (preds_D * W[3]) +\n    (preds_E * W[4])\n)\nprint(\"  ...集成完毕。\")\n\n# === 7. 创建提交文件 ===\nprint(\"⏳ 正在创建 submission.csv...\")\nsubmission_df = pd.DataFrame(final_preds, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\nsubmission_df['id'] = test_df['id']\nsubmission_df = submission_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n\n# 确保概率总和为 1 (SLSQP 优化器会保证总和为1，但保险起见)\nsubmission_df.iloc[:, 1:] = submission_df.iloc[:, 1:].div(submission_df.iloc[:, 1:].sum(axis=1), axis=0)\n# 裁剪以避免 LogLoss 错误\nsubmission_df.iloc[:, 1:] = np.clip(submission_df.iloc[:, 1:], 1e-7, 1 - 1e-7)\n\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"✅ 提交文件创建成功!\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:26:48.992725Z","iopub.execute_input":"2025-10-31T19:26:48.992931Z","iopub.status.idle":"2025-10-31T19:26:49.016802Z","shell.execute_reply.started":"2025-10-31T19:26:48.992917Z","shell.execute_reply":"2025-10-31T19:26:49.016034Z"}},"outputs":[],"execution_count":null}]}